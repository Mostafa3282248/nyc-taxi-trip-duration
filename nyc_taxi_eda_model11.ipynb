{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d8acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dddc219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id2875421          2  2016-03-14 17:24:55  2016-03-14 17:32:30   \n",
       "1  id2377394          1  2016-06-12 00:43:35  2016-06-12 00:54:38   \n",
       "2  id3858529          2  2016-01-19 11:35:24  2016-01-19 12:10:48   \n",
       "3  id3504673          2  2016-04-06 19:32:31  2016-04-06 19:39:40   \n",
       "4  id2181028          2  2016-03-26 13:30:55  2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.765602                  N            455  \n",
       "1         40.731152                  N            663  \n",
       "2         40.710087                  N           2124  \n",
       "3         40.706718                  N            429  \n",
       "4         40.782520                  N            435  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd84f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1458644 entries, 0 to 1458643\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   id                  1458644 non-null  object \n",
      " 1   vendor_id           1458644 non-null  int64  \n",
      " 2   pickup_datetime     1458644 non-null  object \n",
      " 3   dropoff_datetime    1458644 non-null  object \n",
      " 4   passenger_count     1458644 non-null  int64  \n",
      " 5   pickup_longitude    1458644 non-null  float64\n",
      " 6   pickup_latitude     1458644 non-null  float64\n",
      " 7   dropoff_longitude   1458644 non-null  float64\n",
      " 8   dropoff_latitude    1458644 non-null  float64\n",
      " 9   store_and_fwd_flag  1458644 non-null  object \n",
      " 10  trip_duration       1458644 non-null  int64  \n",
      "dtypes: float64(4), int64(3), object(4)\n",
      "memory usage: 122.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1458644, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.info()     # column names + data types + missing values count\n",
    "train.shape      # number of rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10862fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now in: C:\\New folder7\n",
      "Shapes -> train: (1458644, 11)  | test: (625134, 9)\n",
      "\n",
      "=== TRAIN HEAD ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id0801584</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-30 22:01:40</td>\n",
       "      <td>2016-01-30 22:09:03</td>\n",
       "      <td>6</td>\n",
       "      <td>-73.982857</td>\n",
       "      <td>40.742195</td>\n",
       "      <td>-73.992081</td>\n",
       "      <td>40.749184</td>\n",
       "      <td>N</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id1813257</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-17 22:34:59</td>\n",
       "      <td>2016-06-17 22:40:40</td>\n",
       "      <td>4</td>\n",
       "      <td>-73.969017</td>\n",
       "      <td>40.757839</td>\n",
       "      <td>-73.957405</td>\n",
       "      <td>40.765896</td>\n",
       "      <td>N</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id1324603</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-21 07:54:58</td>\n",
       "      <td>2016-05-21 08:20:49</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.969276</td>\n",
       "      <td>40.797779</td>\n",
       "      <td>-73.922470</td>\n",
       "      <td>40.760559</td>\n",
       "      <td>N</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id1301050</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-27 23:12:23</td>\n",
       "      <td>2016-05-27 23:16:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.738400</td>\n",
       "      <td>-73.985786</td>\n",
       "      <td>40.732815</td>\n",
       "      <td>N</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id0012891</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-10 21:45:01</td>\n",
       "      <td>2016-03-10 22:05:26</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.981049</td>\n",
       "      <td>40.744339</td>\n",
       "      <td>-73.973000</td>\n",
       "      <td>40.789989</td>\n",
       "      <td>N</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id2875421          2  2016-03-14 17:24:55  2016-03-14 17:32:30   \n",
       "1  id2377394          1  2016-06-12 00:43:35  2016-06-12 00:54:38   \n",
       "2  id3858529          2  2016-01-19 11:35:24  2016-01-19 12:10:48   \n",
       "3  id3504673          2  2016-04-06 19:32:31  2016-04-06 19:39:40   \n",
       "4  id2181028          2  2016-03-26 13:30:55  2016-03-26 13:38:10   \n",
       "5  id0801584          2  2016-01-30 22:01:40  2016-01-30 22:09:03   \n",
       "6  id1813257          1  2016-06-17 22:34:59  2016-06-17 22:40:40   \n",
       "7  id1324603          2  2016-05-21 07:54:58  2016-05-21 08:20:49   \n",
       "8  id1301050          1  2016-05-27 23:12:23  2016-05-27 23:16:38   \n",
       "9  id0012891          2  2016-03-10 21:45:01  2016-03-10 22:05:26   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "5                6        -73.982857        40.742195         -73.992081   \n",
       "6                4        -73.969017        40.757839         -73.957405   \n",
       "7                1        -73.969276        40.797779         -73.922470   \n",
       "8                1        -73.999481        40.738400         -73.985786   \n",
       "9                1        -73.981049        40.744339         -73.973000   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.765602                  N            455  \n",
       "1         40.731152                  N            663  \n",
       "2         40.710087                  N           2124  \n",
       "3         40.706718                  N            429  \n",
       "4         40.782520                  N            435  \n",
       "5         40.749184                  N            443  \n",
       "6         40.765896                  N            341  \n",
       "7         40.760559                  N           1551  \n",
       "8         40.732815                  N            255  \n",
       "9         40.789989                  N           1225  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST  HEAD ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id3004672</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:58</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.988129</td>\n",
       "      <td>40.732029</td>\n",
       "      <td>-73.990173</td>\n",
       "      <td>40.756680</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id3505355</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:53</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.964203</td>\n",
       "      <td>40.679993</td>\n",
       "      <td>-73.959808</td>\n",
       "      <td>40.655403</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id1217141</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:47</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.997437</td>\n",
       "      <td>40.737583</td>\n",
       "      <td>-73.986160</td>\n",
       "      <td>40.729523</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id2150126</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-30 23:59:41</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.956070</td>\n",
       "      <td>40.771900</td>\n",
       "      <td>-73.986427</td>\n",
       "      <td>40.730469</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id1598245</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:33</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.970215</td>\n",
       "      <td>40.761475</td>\n",
       "      <td>-73.961510</td>\n",
       "      <td>40.755890</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id0668992</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.991302</td>\n",
       "      <td>40.749798</td>\n",
       "      <td>-73.980515</td>\n",
       "      <td>40.786549</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id1765014</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:15</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.978310</td>\n",
       "      <td>40.741550</td>\n",
       "      <td>-73.952072</td>\n",
       "      <td>40.717003</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id0898117</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-30 23:59:09</td>\n",
       "      <td>2</td>\n",
       "      <td>-74.012711</td>\n",
       "      <td>40.701527</td>\n",
       "      <td>-73.986481</td>\n",
       "      <td>40.719509</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id3905224</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-30 23:58:55</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.992332</td>\n",
       "      <td>40.730511</td>\n",
       "      <td>-73.875618</td>\n",
       "      <td>40.875214</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id1543102</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-30 23:58:46</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.993179</td>\n",
       "      <td>40.748760</td>\n",
       "      <td>-73.979309</td>\n",
       "      <td>40.761311</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime  passenger_count  \\\n",
       "0  id3004672          1  2016-06-30 23:59:58                1   \n",
       "1  id3505355          1  2016-06-30 23:59:53                1   \n",
       "2  id1217141          1  2016-06-30 23:59:47                1   \n",
       "3  id2150126          2  2016-06-30 23:59:41                1   \n",
       "4  id1598245          1  2016-06-30 23:59:33                1   \n",
       "5  id0668992          1  2016-06-30 23:59:30                1   \n",
       "6  id1765014          1  2016-06-30 23:59:15                1   \n",
       "7  id0898117          1  2016-06-30 23:59:09                2   \n",
       "8  id3905224          2  2016-06-30 23:58:55                2   \n",
       "9  id1543102          2  2016-06-30 23:58:46                1   \n",
       "\n",
       "   pickup_longitude  pickup_latitude  dropoff_longitude  dropoff_latitude  \\\n",
       "0        -73.988129        40.732029         -73.990173         40.756680   \n",
       "1        -73.964203        40.679993         -73.959808         40.655403   \n",
       "2        -73.997437        40.737583         -73.986160         40.729523   \n",
       "3        -73.956070        40.771900         -73.986427         40.730469   \n",
       "4        -73.970215        40.761475         -73.961510         40.755890   \n",
       "5        -73.991302        40.749798         -73.980515         40.786549   \n",
       "6        -73.978310        40.741550         -73.952072         40.717003   \n",
       "7        -74.012711        40.701527         -73.986481         40.719509   \n",
       "8        -73.992332        40.730511         -73.875618         40.875214   \n",
       "9        -73.993179        40.748760         -73.979309         40.761311   \n",
       "\n",
       "  store_and_fwd_flag  \n",
       "0                  N  \n",
       "1                  N  \n",
       "2                  N  \n",
       "3                  N  \n",
       "4                  N  \n",
       "5                  N  \n",
       "6                  N  \n",
       "7                  N  \n",
       "8                  N  \n",
       "9                  N  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns only in TRAIN: ['dropoff_datetime', 'trip_duration']\n",
      "Columns only in TEST : []\n",
      "\n",
      "=== TRAIN INFO ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                     object\n",
       "vendor_id               int64\n",
       "pickup_datetime        object\n",
       "dropoff_datetime       object\n",
       "passenger_count         int64\n",
       "pickup_longitude      float64\n",
       "pickup_latitude       float64\n",
       "dropoff_longitude     float64\n",
       "dropoff_latitude      float64\n",
       "store_and_fwd_flag     object\n",
       "trip_duration           int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column (TRAIN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "vendor_id             0\n",
       "pickup_datetime       0\n",
       "dropoff_datetime      0\n",
       "passenger_count       0\n",
       "pickup_longitude      0\n",
       "pickup_latitude       0\n",
       "dropoff_longitude     0\n",
       "dropoff_latitude      0\n",
       "store_and_fwd_flag    0\n",
       "trip_duration         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST  INFO ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                     object\n",
       "vendor_id               int64\n",
       "pickup_datetime        object\n",
       "passenger_count         int64\n",
       "pickup_longitude      float64\n",
       "pickup_latitude       float64\n",
       "dropoff_longitude     float64\n",
       "dropoff_latitude      float64\n",
       "store_and_fwd_flag     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column (TEST):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "vendor_id             0\n",
       "pickup_datetime       0\n",
       "passenger_count       0\n",
       "pickup_longitude      0\n",
       "pickup_latitude       0\n",
       "dropoff_longitude     0\n",
       "dropoff_latitude      0\n",
       "store_and_fwd_flag    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datetime-like columns found (TRAIN): ['pickup_datetime', 'dropoff_datetime']\n",
      "Datetime-like columns found (TEST) : ['pickup_datetime']\n",
      "\n",
      "=== TRAIN describe() (numeric) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>1%</th>\n",
       "      <th>5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>95%</th>\n",
       "      <th>99%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vendor_id</th>\n",
       "      <td>1458644.0</td>\n",
       "      <td>1.534950</td>\n",
       "      <td>0.498777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_count</th>\n",
       "      <td>1458644.0</td>\n",
       "      <td>1.664530</td>\n",
       "      <td>1.314242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_longitude</th>\n",
       "      <td>1458644.0</td>\n",
       "      <td>-73.973486</td>\n",
       "      <td>0.070902</td>\n",
       "      <td>-121.933342</td>\n",
       "      <td>-74.014317</td>\n",
       "      <td>-74.006866</td>\n",
       "      <td>-73.991867</td>\n",
       "      <td>-73.981743</td>\n",
       "      <td>-73.967331</td>\n",
       "      <td>-73.891582</td>\n",
       "      <td>-73.782227</td>\n",
       "      <td>-6.133553e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_latitude</th>\n",
       "      <td>1458644.0</td>\n",
       "      <td>40.750921</td>\n",
       "      <td>0.032881</td>\n",
       "      <td>34.359695</td>\n",
       "      <td>40.644825</td>\n",
       "      <td>40.708141</td>\n",
       "      <td>40.737347</td>\n",
       "      <td>40.754101</td>\n",
       "      <td>40.768360</td>\n",
       "      <td>40.788387</td>\n",
       "      <td>40.806599</td>\n",
       "      <td>5.188108e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <td>1458644.0</td>\n",
       "      <td>-73.973416</td>\n",
       "      <td>0.070643</td>\n",
       "      <td>-121.933304</td>\n",
       "      <td>-74.015274</td>\n",
       "      <td>-74.007530</td>\n",
       "      <td>-73.991325</td>\n",
       "      <td>-73.979752</td>\n",
       "      <td>-73.963013</td>\n",
       "      <td>-73.920181</td>\n",
       "      <td>-73.790482</td>\n",
       "      <td>-6.133553e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <td>1458644.0</td>\n",
       "      <td>40.751800</td>\n",
       "      <td>0.035891</td>\n",
       "      <td>32.181141</td>\n",
       "      <td>40.645271</td>\n",
       "      <td>40.699921</td>\n",
       "      <td>40.735885</td>\n",
       "      <td>40.754524</td>\n",
       "      <td>40.769810</td>\n",
       "      <td>40.797508</td>\n",
       "      <td>40.836750</td>\n",
       "      <td>4.392103e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_duration</th>\n",
       "      <td>1458644.0</td>\n",
       "      <td>959.492273</td>\n",
       "      <td>5237.431724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>662.000000</td>\n",
       "      <td>1075.000000</td>\n",
       "      <td>2104.000000</td>\n",
       "      <td>3440.000000</td>\n",
       "      <td>3.526282e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count        mean          std         min         1%  \\\n",
       "vendor_id          1458644.0    1.534950     0.498777    1.000000   1.000000   \n",
       "passenger_count    1458644.0    1.664530     1.314242    0.000000   1.000000   \n",
       "pickup_longitude   1458644.0  -73.973486     0.070902 -121.933342 -74.014317   \n",
       "pickup_latitude    1458644.0   40.750921     0.032881   34.359695  40.644825   \n",
       "dropoff_longitude  1458644.0  -73.973416     0.070643 -121.933304 -74.015274   \n",
       "dropoff_latitude   1458644.0   40.751800     0.035891   32.181141  40.645271   \n",
       "trip_duration      1458644.0  959.492273  5237.431724    1.000000  87.000000   \n",
       "\n",
       "                           5%         25%         50%          75%  \\\n",
       "vendor_id            1.000000    1.000000    2.000000     2.000000   \n",
       "passenger_count      1.000000    1.000000    1.000000     2.000000   \n",
       "pickup_longitude   -74.006866  -73.991867  -73.981743   -73.967331   \n",
       "pickup_latitude     40.708141   40.737347   40.754101    40.768360   \n",
       "dropoff_longitude  -74.007530  -73.991325  -73.979752   -73.963013   \n",
       "dropoff_latitude    40.699921   40.735885   40.754524    40.769810   \n",
       "trip_duration      180.000000  397.000000  662.000000  1075.000000   \n",
       "\n",
       "                           95%          99%           max  \n",
       "vendor_id             2.000000     2.000000  2.000000e+00  \n",
       "passenger_count       5.000000     6.000000  9.000000e+00  \n",
       "pickup_longitude    -73.891582   -73.782227 -6.133553e+01  \n",
       "pickup_latitude      40.788387    40.806599  5.188108e+01  \n",
       "dropoff_longitude   -73.920181   -73.790482 -6.133553e+01  \n",
       "dropoff_latitude     40.797508    40.836750  4.392103e+01  \n",
       "trip_duration      2104.000000  3440.000000  3.526282e+06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Invalid coordinate counts (TRAIN): {'pickup_latitude': np.int64(0), 'pickup_longitude': np.int64(0), 'dropoff_latitude': np.int64(0), 'dropoff_longitude': np.int64(0)}\n",
      "\n",
      "Detected target column: trip_duration\n",
      "Non-positive durations in TRAIN (trip_duration): 0\n",
      "Potential high outliers > 3109.00: 20964\n",
      "Potential low  outliers < 0.00: 0\n",
      "\n",
      "Duplicate IDs in TRAIN (id): 0\n",
      "Duplicate IDs in TEST  (id): 0\n",
      "\n",
      "Top 20 columns by missing ratio (TRAIN):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                    0.0%\n",
       "vendor_id             0.0%\n",
       "pickup_datetime       0.0%\n",
       "dropoff_datetime      0.0%\n",
       "passenger_count       0.0%\n",
       "pickup_longitude      0.0%\n",
       "pickup_latitude       0.0%\n",
       "dropoff_longitude     0.0%\n",
       "dropoff_latitude      0.0%\n",
       "store_and_fwd_flag    0.0%\n",
       "trip_duration         0.0%\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 columns by missing ratio (TEST):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                    0.0%\n",
       "vendor_id             0.0%\n",
       "pickup_datetime       0.0%\n",
       "passenger_count       0.0%\n",
       "pickup_longitude      0.0%\n",
       "pickup_latitude       0.0%\n",
       "dropoff_longitude     0.0%\n",
       "dropoff_latitude      0.0%\n",
       "store_and_fwd_flag    0.0%\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Step 1 audit complete. Review outputs above, then proceed to Step 2 (Feature Engineering).\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 1: Import & Audit the Data\n",
    "# -------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(r\"C:\\New folder7\")  # change to folder where files are\n",
    "print(\"Now in:\", os.getcwd())\n",
    "\n",
    "# 1) Load files (adjust paths if needed)\n",
    "train_path = \"train.csv\"   # or r\"C:\\path\\to\\train.csv\"\n",
    "test_path  = \"test.csv\"    # or r\"C:\\path\\to\\test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test  = pd.read_csv(test_path)\n",
    "\n",
    "print(\"Shapes -> train:\", train.shape, \" | test:\", test.shape)\n",
    "\n",
    "# 2) Quick peeks\n",
    "print(\"\\n=== TRAIN HEAD ===\")\n",
    "display(train.head(10))\n",
    "print(\"\\n=== TEST  HEAD ===\")\n",
    "display(test.head(10))\n",
    "\n",
    "# 3) Column comparison (what’s only in train vs only in test)\n",
    "train_cols = set(train.columns)\n",
    "test_cols  = set(test.columns)\n",
    "print(\"\\nColumns only in TRAIN:\", sorted(list(train_cols - test_cols)))\n",
    "print(\"Columns only in TEST :\", sorted(list(test_cols - train_cols)))\n",
    "\n",
    "# 4) Dtypes & non-null counts\n",
    "print(\"\\n=== TRAIN INFO ===\")\n",
    "display(train.dtypes)\n",
    "print(\"\\nMissing values per column (TRAIN):\")\n",
    "display(train.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\n=== TEST  INFO ===\")\n",
    "display(test.dtypes)\n",
    "print(\"\\nMissing values per column (TEST):\")\n",
    "display(test.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# 5) Try to parse common datetime columns if present\n",
    "dt_candidates = [c for c in train.columns if \"time\" in c.lower() or \"date\" in c.lower()]\n",
    "for c in dt_candidates:\n",
    "    try:\n",
    "        train[c] = pd.to_datetime(train[c], errors=\"coerce\", utc=True)\n",
    "    except Exception: \n",
    "        pass\n",
    "dt_candidates_test = [c for c in test.columns if \"time\" in c.lower() or \"date\" in c.lower()]\n",
    "for c in dt_candidates_test:\n",
    "    try:\n",
    "        test[c] = pd.to_datetime(test[c], errors=\"coerce\", utc=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"\\nDatetime-like columns found (TRAIN):\", dt_candidates)\n",
    "print(\"Datetime-like columns found (TEST) :\", dt_candidates_test)\n",
    "\n",
    "# 6) Basic statistics (numerical columns)\n",
    "print(\"\\n=== TRAIN describe() (numeric) ===\")\n",
    "display(train.describe(percentiles=[.01,.05,.25,.5,.75,.95,.99]).T)\n",
    "\n",
    "# 7) Sanity checks for coordinates and duration (if present)\n",
    "def has_any(cols, names):\n",
    "    return any(name in cols for name in names)\n",
    "\n",
    "lat_names  = [\"pickup_latitude\",\"dropoff_latitude\",\"lat\",\"latitude\"]\n",
    "lon_names  = [\"pickup_longitude\",\"dropoff_longitude\",\"lon\",\"longitude\",\"lng\"]\n",
    "\n",
    "# Helper to find first matching column by candidates\n",
    "def find_col(cols, candidates):\n",
    "    for name in candidates:\n",
    "        if name in cols:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "train_cols_list = list(train.columns)\n",
    "test_cols_list  = list(test.columns)\n",
    "\n",
    "# Try to detect coordinate columns\n",
    "pickup_lat  = find_col(train_cols_list, [\"pickup_latitude\",\"start_lat\",\"lat_pickup\",\"pickup_lat\"])\n",
    "pickup_lon  = find_col(train_cols_list, [\"pickup_longitude\",\"start_lon\",\"lng_pickup\",\"pickup_lon\"])\n",
    "dropoff_lat = find_col(train_cols_list, [\"dropoff_latitude\",\"end_lat\",\"lat_dropoff\",\"dropoff_lat\"])\n",
    "dropoff_lon = find_col(train_cols_list, [\"dropoff_longitude\",\"end_lon\",\"lng_dropoff\",\"dropoff_lon\"])\n",
    "\n",
    "coord_cols = [c for c in [pickup_lat,pickup_lon,dropoff_lat,dropoff_lon] if c is not None]\n",
    "if coord_cols:\n",
    "    # Valid ranges: lat in [-90, 90], lon in [-180, 180]\n",
    "    def invalid_lat(s): return (~s.between(-90, 90)) | s.isna()\n",
    "    def invalid_lon(s): return (~s.between(-180, 180)) | s.isna()\n",
    "\n",
    "    invalid_counts = {}\n",
    "    if pickup_lat:  invalid_counts[pickup_lat]  = invalid_lat(train[pickup_lat]).sum()\n",
    "    if pickup_lon:  invalid_counts[pickup_lon]  = invalid_lon(train[pickup_lon]).sum()\n",
    "    if dropoff_lat: invalid_counts[dropoff_lat] = invalid_lat(train[dropoff_lat]).sum()\n",
    "    if dropoff_lon: invalid_counts[dropoff_lon] = invalid_lon(train[dropoff_lon]).sum()\n",
    "\n",
    "    print(\"\\nInvalid coordinate counts (TRAIN):\", invalid_counts)\n",
    "\n",
    "# 8) Target existence and quick checks\n",
    "# Common target names for trip time\n",
    "possible_targets = [\"trip_duration\",\"duration\",\"travel_time\",\"period_of_trip\",\"trip_time_min\",\"trip_time_sec\"]\n",
    "target_col = find_col(train_cols_list, possible_targets)\n",
    "\n",
    "print(\"\\nDetected target column:\", target_col)\n",
    "\n",
    "if target_col:\n",
    "    # Negative/zero durations, extreme outliers\n",
    "    neg_or_zero = (train[target_col] <= 0).sum()\n",
    "    print(f\"Non-positive durations in TRAIN ({target_col}):\", neg_or_zero)\n",
    "\n",
    "    # Basic outlier flagging by percentile\n",
    "    q1 = train[target_col].quantile(0.25)\n",
    "    q3 = train[target_col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    hi = q3 + 3*iqr\n",
    "    lo = max(q1 - 3*iqr, 0)\n",
    "    outlier_hi = (train[target_col] > hi).sum()\n",
    "    outlier_lo = (train[target_col] < lo).sum()\n",
    "    print(f\"Potential high outliers > {hi:.2f}: {outlier_hi}\")\n",
    "    print(f\"Potential low  outliers < {lo:.2f}: {outlier_lo}\")\n",
    "\n",
    "# 9) Duplicate checks (by id if present, else full row)\n",
    "id_col = find_col(train_cols_list, [\"id\",\"trip_id\",\"ride_id\"])\n",
    "if id_col:\n",
    "    dup_ids_train = train[id_col].duplicated().sum()\n",
    "    print(f\"\\nDuplicate IDs in TRAIN ({id_col}):\", dup_ids_train)\n",
    "    if id_col in test.columns:\n",
    "        dup_ids_test = test[id_col].duplicated().sum()\n",
    "        print(f\"Duplicate IDs in TEST  ({id_col}):\", dup_ids_test)\n",
    "else:\n",
    "    print(\"\\nNo obvious ID column found — skipping ID-duplicate check.\")\n",
    "    print(\"Row-level duplicates (TRAIN):\", train.duplicated().sum())\n",
    "    print(\"Row-level duplicates (TEST) :\", test.duplicated().sum())\n",
    "\n",
    "# 10) Quick null heatmap hint (optional visualization later):\n",
    "# You can visualize with: train.isna().mean().sort_values(ascending=False).head(20)\n",
    "print(\"\\nTop 20 columns by missing ratio (TRAIN):\")\n",
    "display((train.isna().mean().sort_values(ascending=False).head(20) * 100).round(2).astype(str) + \"%\")\n",
    "\n",
    "print(\"\\nTop 20 columns by missing ratio (TEST):\")\n",
    "display((test.isna().mean().sort_values(ascending=False).head(20) * 100).round(2).astype(str) + \"%\")\n",
    "\n",
    "print(\"\\n✅ Step 1 audit complete. Review outputs above, then proceed to Step 2 (Feature Engineering).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45066c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected -> id: id | target: trip_duration | pickup_lat: pickup_latitude | pickup_lon: pickup_longitude | dropoff_lat: dropoff_latitude | dropoff_lon: dropoff_longitude | datetime(train/test): pickup_datetime pickup_datetime\n",
      "Removed non-positive durations: 0\n",
      "Clipped high tail at 99.5%: 4139.00\n",
      "✅ Feature engineering complete.\n",
      "Saved: train_fe.csv -> (1458644, 22) | test_fe.csv -> (625134, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>wday_sin</th>\n",
       "      <th>wday_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>...</th>\n",
       "      <th>manhattan_km</th>\n",
       "      <th>bearing_deg</th>\n",
       "      <th>lat_diff</th>\n",
       "      <th>lon_diff</th>\n",
       "      <th>flag_invalid_pickup</th>\n",
       "      <th>flag_invalid_dropoff</th>\n",
       "      <th>flag_zero_distance</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>target_log1p</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.735670</td>\n",
       "      <td>99.970196</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.017525</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.122493</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>2.427800</td>\n",
       "      <td>242.846232</td>\n",
       "      <td>0.007412</td>\n",
       "      <td>0.019066</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.498282</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>8.173527</td>\n",
       "      <td>200.319835</td>\n",
       "      <td>0.053852</td>\n",
       "      <td>0.026306</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.661527</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653332</td>\n",
       "      <td>187.262300</td>\n",
       "      <td>0.013252</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.063785</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.192833</td>\n",
       "      <td>179.473585</td>\n",
       "      <td>0.010689</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.077642</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id0801584</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.550674</td>\n",
       "      <td>315.004404</td>\n",
       "      <td>0.006989</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.095825</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id1813257</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.869940</td>\n",
       "      <td>47.505775</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>0.011612</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.834811</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id1324603</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>8.061111</td>\n",
       "      <td>136.385396</td>\n",
       "      <td>0.037220</td>\n",
       "      <td>0.046806</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.347300</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id1301050</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.772683</td>\n",
       "      <td>118.284067</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>0.013695</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.545177</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id0012891</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.726371</td>\n",
       "      <td>7.603538</td>\n",
       "      <td>0.045650</td>\n",
       "      <td>0.008049</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.111512</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  hour  weekday  month  is_weekend  hour_sin  hour_cos  wday_sin  \\\n",
       "0  id2875421    17        0      3           0 -0.965926 -0.258819  0.000000   \n",
       "1  id2377394     0        6      6           1  0.000000  1.000000 -0.781831   \n",
       "2  id3858529    11        1      1           0  0.258819 -0.965926  0.781831   \n",
       "3  id3504673    19        2      4           0 -0.965926  0.258819  0.974928   \n",
       "4  id2181028    13        5      3           1 -0.258819 -0.965926 -0.974928   \n",
       "5  id0801584    22        5      1           1 -0.500000  0.866025 -0.974928   \n",
       "6  id1813257    22        4      6           0 -0.500000  0.866025 -0.433884   \n",
       "7  id1324603     7        5      5           1  0.965926 -0.258819 -0.974928   \n",
       "8  id1301050    23        4      5           0 -0.258819  0.965926 -0.433884   \n",
       "9  id0012891    21        3      3           0 -0.707107  0.707107  0.433884   \n",
       "\n",
       "   wday_cos     month_sin  ...  manhattan_km  bearing_deg  lat_diff  lon_diff  \\\n",
       "0  1.000000  1.000000e+00  ...      1.735670    99.970196  0.002335  0.017525   \n",
       "1  0.623490  1.224647e-16  ...      2.427800   242.846232  0.007412  0.019066   \n",
       "2  0.623490  5.000000e-01  ...      8.173527   200.319835  0.053852  0.026306   \n",
       "3 -0.222521  8.660254e-01  ...      1.653332   187.262300  0.013252  0.002228   \n",
       "4 -0.222521  1.000000e+00  ...      1.192833   179.473585  0.010689  0.000130   \n",
       "5 -0.222521  5.000000e-01  ...      1.550674   315.004404  0.006989  0.009224   \n",
       "6 -0.900969  1.224647e-16  ...      1.869940    47.505775  0.008057  0.011612   \n",
       "7 -0.222521  5.000000e-01  ...      8.061111   136.385396  0.037220  0.046806   \n",
       "8 -0.900969  5.000000e-01  ...      1.772683   118.284067  0.005585  0.013695   \n",
       "9 -0.900969  1.000000e+00  ...      5.726371     7.603538  0.045650  0.008049   \n",
       "\n",
       "   flag_invalid_pickup  flag_invalid_dropoff  flag_zero_distance  vendor_id  \\\n",
       "0                False                 False                   0          2   \n",
       "1                False                 False                   0          1   \n",
       "2                False                 False                   0          2   \n",
       "3                False                 False                   0          2   \n",
       "4                False                 False                   0          2   \n",
       "5                False                 False                   0          2   \n",
       "6                False                 False                   0          1   \n",
       "7                False                 False                   0          2   \n",
       "8                False                 False                   0          1   \n",
       "9                False                 False                   0          2   \n",
       "\n",
       "   target_log1p trip_duration  \n",
       "0      6.122493           455  \n",
       "1      6.498282           663  \n",
       "2      7.661527          2124  \n",
       "3      6.063785           429  \n",
       "4      6.077642           435  \n",
       "5      6.095825           443  \n",
       "6      5.834811           341  \n",
       "7      7.347300          1551  \n",
       "8      5.545177           255  \n",
       "9      7.111512          1225  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>wday_sin</th>\n",
       "      <th>wday_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>haversine_km</th>\n",
       "      <th>manhattan_km</th>\n",
       "      <th>bearing_deg</th>\n",
       "      <th>lat_diff</th>\n",
       "      <th>lon_diff</th>\n",
       "      <th>flag_invalid_pickup</th>\n",
       "      <th>flag_invalid_dropoff</th>\n",
       "      <th>flag_zero_distance</th>\n",
       "      <th>vendor_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id3004672</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.746430</td>\n",
       "      <td>2.898159</td>\n",
       "      <td>356.404776</td>\n",
       "      <td>0.024651</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id3505355</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.759243</td>\n",
       "      <td>3.090022</td>\n",
       "      <td>172.278835</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.004395</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id1217141</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.306157</td>\n",
       "      <td>1.842462</td>\n",
       "      <td>133.326248</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id2150126</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.269095</td>\n",
       "      <td>7.141294</td>\n",
       "      <td>209.043167</td>\n",
       "      <td>0.041431</td>\n",
       "      <td>0.030357</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id1598245</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.960843</td>\n",
       "      <td>1.351552</td>\n",
       "      <td>130.260381</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id0668992</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.186278</td>\n",
       "      <td>4.973206</td>\n",
       "      <td>12.530495</td>\n",
       "      <td>0.036751</td>\n",
       "      <td>0.010788</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id1765014</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.512619</td>\n",
       "      <td>4.927677</td>\n",
       "      <td>140.985013</td>\n",
       "      <td>0.024548</td>\n",
       "      <td>0.026237</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id0898117</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.980958</td>\n",
       "      <td>4.201728</td>\n",
       "      <td>47.864234</td>\n",
       "      <td>0.017982</td>\n",
       "      <td>0.026230</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id3905224</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>18.852197</td>\n",
       "      <td>25.835332</td>\n",
       "      <td>31.368065</td>\n",
       "      <td>0.144703</td>\n",
       "      <td>0.116714</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id1543102</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.820017</td>\n",
       "      <td>2.557361</td>\n",
       "      <td>39.930618</td>\n",
       "      <td>0.012550</td>\n",
       "      <td>0.013870</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  hour  weekday  month  is_weekend  hour_sin  hour_cos  wday_sin  \\\n",
       "0  id3004672    23        3      6           0 -0.258819  0.965926  0.433884   \n",
       "1  id3505355    23        3      6           0 -0.258819  0.965926  0.433884   \n",
       "2  id1217141    23        3      6           0 -0.258819  0.965926  0.433884   \n",
       "3  id2150126    23        3      6           0 -0.258819  0.965926  0.433884   \n",
       "4  id1598245    23        3      6           0 -0.258819  0.965926  0.433884   \n",
       "5  id0668992    23        3      6           0 -0.258819  0.965926  0.433884   \n",
       "6  id1765014    23        3      6           0 -0.258819  0.965926  0.433884   \n",
       "7  id0898117    23        3      6           0 -0.258819  0.965926  0.433884   \n",
       "8  id3905224    23        3      6           0 -0.258819  0.965926  0.433884   \n",
       "9  id1543102    23        3      6           0 -0.258819  0.965926  0.433884   \n",
       "\n",
       "   wday_cos     month_sin  month_cos  haversine_km  manhattan_km  bearing_deg  \\\n",
       "0 -0.900969  1.224647e-16       -1.0      2.746430      2.898159   356.404776   \n",
       "1 -0.900969  1.224647e-16       -1.0      2.759243      3.090022   172.278835   \n",
       "2 -0.900969  1.224647e-16       -1.0      1.306157      1.842462   133.326248   \n",
       "3 -0.900969  1.224647e-16       -1.0      5.269095      7.141294   209.043167   \n",
       "4 -0.900969  1.224647e-16       -1.0      0.960843      1.351552   130.260381   \n",
       "5 -0.900969  1.224647e-16       -1.0      4.186278      4.973206    12.530495   \n",
       "6 -0.900969  1.224647e-16       -1.0      3.512619      4.927677   140.985013   \n",
       "7 -0.900969  1.224647e-16       -1.0      2.980958      4.201728    47.864234   \n",
       "8 -0.900969  1.224647e-16       -1.0     18.852197     25.835332    31.368065   \n",
       "9 -0.900969  1.224647e-16       -1.0      1.820017      2.557361    39.930618   \n",
       "\n",
       "   lat_diff  lon_diff  flag_invalid_pickup  flag_invalid_dropoff  \\\n",
       "0  0.024651  0.002045                False                 False   \n",
       "1  0.024590  0.004395                False                 False   \n",
       "2  0.008060  0.011276                False                 False   \n",
       "3  0.041431  0.030357                False                 False   \n",
       "4  0.005585  0.008705                False                 False   \n",
       "5  0.036751  0.010788                False                 False   \n",
       "6  0.024548  0.026237                False                 False   \n",
       "7  0.017982  0.026230                False                 False   \n",
       "8  0.144703  0.116714                False                 False   \n",
       "9  0.012550  0.013870                False                 False   \n",
       "\n",
       "   flag_zero_distance vendor_id  \n",
       "0                   0         1  \n",
       "1                   0         1  \n",
       "2                   0         1  \n",
       "3                   0         2  \n",
       "4                   0         1  \n",
       "5                   0         1  \n",
       "6                   0         1  \n",
       "7                   0         1  \n",
       "8                   0         2  \n",
       "9                   0         2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------------------\n",
    "# STEP 2: Feature Engineering (Trip Time)\n",
    "# --------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "train_path = \"train.csv\"\n",
    "test_path  = \"test.csv\"\n",
    "\n",
    "# Candidate names so code adapts to your schema\n",
    "CAND_ID          = [\"id\",\"trip_id\",\"ride_id\",\"row_id\"]\n",
    "CAND_TARGET      = [\"trip_duration\",\"duration\",\"travel_time\",\"period_of_trip\",\n",
    "                    \"trip_time_min\",\"trip_time_sec\"]\n",
    "CAND_PICKUP_LAT  = [\"pickup_latitude\",\"start_lat\",\"lat_pickup\",\"pickup_lat\"]\n",
    "CAND_PICKUP_LON  = [\"pickup_longitude\",\"start_lon\",\"lng_pickup\",\"pickup_lon\",\"pickup_longitude\"]\n",
    "CAND_DROPOFF_LAT = [\"dropoff_latitude\",\"end_lat\",\"lat_dropoff\",\"dropoff_lat\"]\n",
    "CAND_DROPOFF_LON = [\"dropoff_longitude\",\"end_lon\",\"lng_dropoff\",\"dropoff_lon\",\"dropoff_longitude\"]\n",
    "CAND_DT          = [\"pickup_datetime\",\"start_time\",\"start_datetime\",\"datetime\",\"timestamp\",\"trip_start_time\"]\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def find_col(cols, candidates):\n",
    "    cols_lower = {c.lower(): c for c in cols}\n",
    "    for name in candidates:\n",
    "        if name.lower() in cols_lower:\n",
    "            return cols_lower[name.lower()]\n",
    "    return None\n",
    "\n",
    "def to_datetime_col(df, candidates):\n",
    "    c = find_col(df.columns, candidates)\n",
    "    if c is None:\n",
    "        return None\n",
    "    df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
    "    return c\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    # All args in degrees\n",
    "    R = 6371.0088  # mean Earth radius in km\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlmb = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlmb/2)**2\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "def bearing_deg(lat1, lon1, lat2, lon2):\n",
    "    # Forward azimuth (0-360)\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    dlmb = np.radians(lon2 - lon1)\n",
    "    y = np.sin(dlmb) * np.cos(phi2)\n",
    "    x = np.cos(phi1)*np.sin(phi2) - np.sin(phi1)*np.cos(phi2)*np.cos(dlmb)\n",
    "    brng = np.degrees(np.arctan2(y, x))\n",
    "    return (brng + 360) % 360\n",
    "\n",
    "def clip_series(s, lo=None, hi=None):\n",
    "    if lo is not None: s = s.clip(lower=lo)\n",
    "    if hi is not None: s = s.clip(upper=hi)\n",
    "    return s\n",
    "\n",
    "def build_time_features(df, dt_col):\n",
    "    # hour, weekday, month, weekend + cyclical encodings\n",
    "    df[\"hour\"]       = df[dt_col].dt.hour\n",
    "    df[\"weekday\"]    = df[dt_col].dt.weekday   # 0=Mon\n",
    "    df[\"month\"]      = df[dt_col].dt.month\n",
    "    df[\"is_weekend\"] = df[\"weekday\"].isin([5,6]).astype(int)\n",
    "\n",
    "    # Cyclic transforms\n",
    "    df[\"hour_sin\"]   = np.sin(2*np.pi*df[\"hour\"]/24.0)\n",
    "    df[\"hour_cos\"]   = np.cos(2*np.pi*df[\"hour\"]/24.0)\n",
    "    df[\"wday_sin\"]   = np.sin(2*np.pi*df[\"weekday\"]/7.0)\n",
    "    df[\"wday_cos\"]   = np.cos(2*np.pi*df[\"weekday\"]/7.0)\n",
    "    df[\"month_sin\"]  = np.sin(2*np.pi*df[\"month\"]/12.0)\n",
    "    df[\"month_cos\"]  = np.cos(2*np.pi*df[\"month\"]/12.0)\n",
    "\n",
    "def build_distance_features(df, p_lat, p_lon, d_lat, d_lon):\n",
    "    # Straight line distance (Haversine)\n",
    "    df[\"haversine_km\"] = haversine_km(df[p_lat], df[p_lon], df[d_lat], df[d_lon])\n",
    "    # Manhattan-like proxy on sphere: project deltas\n",
    "    # (not exact road distance, but useful signal)\n",
    "    df[\"lat_diff\"] = (df[d_lat] - df[p_lat]).abs()\n",
    "    df[\"lon_diff\"] = (df[d_lon] - df[p_lon]).abs()\n",
    "    # Scale lon_diff roughly by latitude to reflect km scale at that latitude\n",
    "    mean_lat_rad = np.radians((df[p_lat] + df[d_lat]) / 2.0)\n",
    "    km_per_deg_lat = 110.574\n",
    "    km_per_deg_lon = 111.320 * np.cos(mean_lat_rad)\n",
    "    man_lat_km = df[\"lat_diff\"] * km_per_deg_lat\n",
    "    man_lon_km = df[\"lon_diff\"] * km_per_deg_lon\n",
    "    df[\"manhattan_km\"] = man_lat_km + man_lon_km\n",
    "    # Bearing (direction)\n",
    "    df[\"bearing_deg\"] = bearing_deg(df[p_lat], df[p_lon], df[d_lat], df[d_lon])\n",
    "\n",
    "def basic_quality_flags(df, p_lat, p_lon, d_lat, d_lon):\n",
    "    # Valid geographic ranges\n",
    "    df[\"flag_invalid_pickup\"]  = (~df[p_lat].between(-90, 90)) | (~df[p_lon].between(-180, 180))\n",
    "    df[\"flag_invalid_dropoff\"] = (~df[d_lat].between(-90, 90)) | (~df[d_lon].between(-180, 180))\n",
    "    df[\"flag_zero_distance\"]   = (df[\"haversine_km\"] == 0).astype(int)\n",
    "\n",
    "# ---------- LOAD ----------\n",
    "train = pd.read_csv(train_path)\n",
    "test  = pd.read_csv(test_path)\n",
    "\n",
    "# ---------- IDENTIFY KEY COLUMNS ----------\n",
    "id_col      = find_col(train.columns, CAND_ID) or find_col(test.columns, CAND_ID)\n",
    "target_col  = find_col(train.columns, CAND_TARGET)\n",
    "pickup_lat  = find_col(train.columns, CAND_PICKUP_LAT)\n",
    "pickup_lon  = find_col(train.columns, CAND_PICKUP_LON)\n",
    "dropoff_lat = find_col(train.columns, CAND_DROPOFF_LAT)\n",
    "dropoff_lon = find_col(train.columns, CAND_DROPOFF_LON)\n",
    "\n",
    "# Parse a datetime column (if present)\n",
    "dt_col_train = to_datetime_col(train, CAND_DT)\n",
    "dt_col_test  = to_datetime_col(test,  CAND_DT)\n",
    "\n",
    "print(\"Detected -> id:\", id_col, \"| target:\", target_col, \n",
    "      \"| pickup_lat:\", pickup_lat, \"| pickup_lon:\", pickup_lon, \n",
    "      \"| dropoff_lat:\", dropoff_lat, \"| dropoff_lon:\", dropoff_lon, \n",
    "      \"| datetime(train/test):\", dt_col_train, dt_col_test)\n",
    "\n",
    "# ---------- FEATURE ENGINEERING ----------\n",
    "# 1) Time features (if datetime exists)\n",
    "if dt_col_train: build_time_features(train, dt_col_train)\n",
    "if dt_col_test:  build_time_features(test,  dt_col_test)\n",
    "\n",
    "# 2) Distance/geospatial features (if coords exist)\n",
    "if all([pickup_lat, pickup_lon, dropoff_lat, dropoff_lon]):\n",
    "    build_distance_features(train, pickup_lat, pickup_lon, dropoff_lat, dropoff_lon)\n",
    "    build_distance_features(test,  pickup_lat, pickup_lon, dropoff_lat, dropoff_lon)\n",
    "    basic_quality_flags(train, pickup_lat, pickup_lon, dropoff_lat, dropoff_lon)\n",
    "    basic_quality_flags(test,  pickup_lat, pickup_lon, dropoff_lat, dropoff_lon)\n",
    "else:\n",
    "    # If no coords, try to use any existing 'distance_km' column as-is\n",
    "    if \"distance_km\" not in train.columns:\n",
    "        print(\"⚠️ No coordinate columns found; distance features skipped.\")\n",
    "\n",
    "# 3) Safe target prep (train only)\n",
    "if target_col:\n",
    "    # Convert minutes to seconds if name suggests, else leave; just ensure numeric\n",
    "    train[target_col] = pd.to_numeric(train[target_col], errors=\"coerce\")\n",
    "\n",
    "    # Remove obviously invalid target rows (train only)\n",
    "    # Keep a copy of raw target; also prepare log1p target for models that prefer it\n",
    "    train[\"target_raw\"] = train[target_col]\n",
    "\n",
    "    # Non-positive durations are unusable\n",
    "    mask_valid_t = train[target_col] > 0\n",
    "    removed_nonpos = (~mask_valid_t).sum()\n",
    "    train = train[mask_valid_t].copy()\n",
    "\n",
    "    # Clip *extreme* outliers on the high side to stabilize models\n",
    "    hi = train[target_col].quantile(0.995)  # 99.5th percentile\n",
    "    train[\"target_clipped\"] = clip_series(train[target_col], lo=None, hi=hi)\n",
    "    train[\"target_log1p\"]   = np.log1p(train[\"target_clipped\"])\n",
    "\n",
    "    print(f\"Removed non-positive durations: {removed_nonpos}\")\n",
    "    print(f\"Clipped high tail at 99.5%: {hi:.2f}\")\n",
    "else:\n",
    "    print(\"⚠️ No target column detected in TRAIN — expected in train only; continuing.\")\n",
    "\n",
    "# 4) Minimal categorical handling (example: vendor_id if exists)\n",
    "for cat_col in [\"vendor_id\",\"route_type\",\"service_level\"]:\n",
    "    if cat_col in train.columns:\n",
    "        train[cat_col] = train[cat_col].astype(\"category\")\n",
    "    if cat_col in test.columns:\n",
    "        test[cat_col] = test[cat_col].astype(\"category\")\n",
    "\n",
    "# 5) Final column selection for modeling\n",
    "#    Keep: ids (if exist), engineered features, and targets (train only)\n",
    "keep_features = [\n",
    "    # IDs\n",
    "    id_col if id_col in (train.columns if id_col else []) else None,\n",
    "    # Time\n",
    "    \"hour\",\"weekday\",\"month\",\"is_weekend\",\"hour_sin\",\"hour_cos\",\"wday_sin\",\"wday_cos\",\"month_sin\",\"month_cos\",\n",
    "    # Geo\n",
    "    \"haversine_km\",\"manhattan_km\",\"bearing_deg\",\"lat_diff\",\"lon_diff\",\n",
    "    # Quality flags\n",
    "    \"flag_invalid_pickup\",\"flag_invalid_dropoff\",\"flag_zero_distance\",\n",
    "]\n",
    "keep_features = [c for c in keep_features if c and c in train.columns]\n",
    "\n",
    "# Add any categorical columns we coerced\n",
    "for cat_col in [\"vendor_id\",\"route_type\",\"service_level\"]:\n",
    "    if cat_col in train.columns:\n",
    "        keep_features.append(cat_col)\n",
    "\n",
    "# Add targets for train\n",
    "train_targets = []\n",
    "if \"target_log1p\" in train.columns: train_targets.append(\"target_log1p\")\n",
    "if target_col and target_col in train.columns: train_targets.append(target_col)\n",
    "\n",
    "# Build final frames\n",
    "train_fe = train[[c for c in keep_features if c in train.columns] + train_targets].copy()\n",
    "test_fe_cols = [c for c in keep_features if c in test.columns]\n",
    "test_fe  = test[test_fe_cols].copy()\n",
    "\n",
    "# If ID exists but was not in keep_features, add it for merging/submission\n",
    "if id_col and id_col in train.columns and id_col not in train_fe.columns:\n",
    "    train_fe[id_col] = train[id_col]\n",
    "if id_col and id_col in test.columns and id_col not in test_fe.columns:\n",
    "    test_fe[id_col] = test[id_col]\n",
    "\n",
    "# Reorder columns (ID first if present)\n",
    "def reorder_with_id_first(df, id_col_name):\n",
    "    if id_col_name and id_col_name in df.columns:\n",
    "        cols = [id_col_name] + [c for c in df.columns if c != id_col_name]\n",
    "        return df[cols]\n",
    "    return df\n",
    "\n",
    "train_fe = reorder_with_id_first(train_fe, id_col)\n",
    "test_fe  = reorder_with_id_first(test_fe,  id_col)\n",
    "\n",
    "# Save engineered datasets\n",
    "train_fe_path = \"train_fe.csv\"\n",
    "test_fe_path  = \"test_fe.csv\"\n",
    "train_fe.to_csv(train_fe_path, index=False)\n",
    "test_fe.to_csv(test_fe_path,  index=False)\n",
    "\n",
    "print(\"✅ Feature engineering complete.\")\n",
    "print(\"Saved:\", train_fe_path, \"->\", train_fe.shape, \"|\", test_fe_path, \"->\", test_fe.shape)\n",
    "\n",
    "# Optional: quick preview\n",
    "display(train_fe.head(10))\n",
    "display(test_fe.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a01287c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\New folder7\n",
      "Expecting: C:\\New folder7\\train_fe.csv and C:\\New folder7\\test_fe.csv\n",
      "Loaded -> train: (1458644, 22) | test: (625134, 20)\n",
      "Detected -> id: id | target: target_log1p | use_log_target=True\n",
      "Split -> X_tr: (1166915, 19)  | X_val: (291729, 19)\n",
      "RF: {'MAE': 209.0416946675643, 'RMSE': 341.48042675457043} | time=3319.4s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.148876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's l2: 0.158182\n",
      "[400]\tvalid_0's l2: 0.157772\n",
      "[600]\tvalid_0's l2: 0.157736\n",
      "Early stopping, best iteration is:\n",
      "[678]\tvalid_0's l2: 0.157686\n",
      "LGBM: {'MAE': 203.72192715712453, 'RMSE': 336.7015529514449} | best_iter=678 | time=730.4s\n",
      "[0]\ttrain-rmse:0.74946\teval-rmse:0.74939\n",
      "[100]\ttrain-rmse:0.39489\teval-rmse:0.39772\n",
      "[200]\ttrain-rmse:0.38753\teval-rmse:0.39656\n",
      "[287]\ttrain-rmse:0.38297\teval-rmse:0.39710\n",
      "XGB: {'MAE': 204.19839837974467, 'RMSE': 337.51905395793517} | best_iter=187 | time=152.32s\n",
      "\n",
      "Validation scores (lower is better):\n",
      "RandomForest -> MAE: 209.0417 | RMSE: 341.4804\n",
      "LightGBM     -> MAE: 203.7219 | RMSE: 336.7016\n",
      "XGBoost      -> MAE: 204.1984 | RMSE: 337.5191\n",
      "Ensemble     -> MAE: 203.7181 | RMSE: 336.8190\n",
      "\n",
      "🏆 Best model by RMSE: LightGBM\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.480863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1458644, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.462139\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 265\u001b[39m\n\u001b[32m    261\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (m1.predict(X_test) + bst_full.predict(dtest)) / \u001b[32m2.0\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnknown model name.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m pred_test = \u001b[43mrefit_and_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# Inverse-transform if trained on log target\u001b[39;00m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_log_target:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 230\u001b[39m, in \u001b[36mrefit_and_predict\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m m.predict(X_test)\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mLightGBM\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    225\u001b[39m     m = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBMRegressor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlgbm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbest_iteration_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m800\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mFAST_DEBUG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.10\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mFAST_DEBUG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_leaves\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m48\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mFAST_DEBUG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubsample\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolsample_bytree\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m m.predict(X_test)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    234\u001b[39m     \u001b[38;5;66;03m# Refit using native API with best iteration from validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\sklearn.py:1398\u001b[39m, in \u001b[36mLGBMRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1383\u001b[39m     X: _LGBM_ScikitMatrixLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1395\u001b[39m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1396\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLGBMRegressor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1397\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1398\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# STEP 3 — Modeling, Evaluation, Submission (final build)\n",
    "# XGBoost via native API (no sklearn callbacks issues)\n",
    "# Compatible with: xgboost 3.x, scikit-learn 1.6.x\n",
    "# ==========================================================\n",
    "import os, time, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------\n",
    "# (A) Settings & Paths\n",
    "# --------------------------\n",
    "BASE = os.getcwd()  # change to absolute path if needed, e.g. r\"C:\\New folder7\"\n",
    "train_fe_path = os.path.join(BASE, \"train_fe.csv\")\n",
    "test_fe_path  = os.path.join(BASE, \"test_fe.csv\")\n",
    "sample_path   = os.path.join(BASE, \"sample_submission.csv\")\n",
    "\n",
    "FAST_DEBUG   = False# set to False for best accuracy on full data\n",
    "N_ROWS_DEBUG = 200_000   # rows to sample in fast mode\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Expecting:\", train_fe_path, \"and\", test_fe_path)\n",
    "\n",
    "# --------------------------\n",
    "# (B) Load engineered data\n",
    "# --------------------------\n",
    "train = pd.read_csv(train_fe_path)\n",
    "test  = pd.read_csv(test_fe_path)\n",
    "print(\"Loaded -> train:\", train.shape, \"| test:\", test.shape)\n",
    "\n",
    "# ----------------------------------\n",
    "# (C) Detect ID and target columns\n",
    "# ----------------------------------\n",
    "CAND_ID     = [\"id\",\"trip_id\",\"ride_id\",\"row_id\"]\n",
    "CAND_TARGET = [\"target_log1p\",\"trip_duration\",\"duration\",\"travel_time\",\n",
    "               \"period_of_trip\",\"trip_time_min\",\"trip_time_sec\"]\n",
    "\n",
    "def find_col(cols, candidates):\n",
    "    low = {c.lower(): c for c in cols}\n",
    "    for name in candidates:\n",
    "        if name.lower() in low:\n",
    "            return low[name.lower()]\n",
    "    return None\n",
    "\n",
    "id_col     = find_col(train.columns, CAND_ID) or find_col(test.columns, CAND_ID)\n",
    "target_col = \"target_log1p\" if \"target_log1p\" in train.columns else find_col(train.columns, CAND_TARGET)\n",
    "if target_col is None:\n",
    "    raise ValueError(\"No target column found (expected 'target_log1p' or a duration column).\")\n",
    "\n",
    "use_log_target = (target_col == \"target_log1p\")\n",
    "print(f\"Detected -> id: {id_col} | target: {target_col} | use_log_target={use_log_target}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# (D) Feature set (drop targets and non-feature cols)\n",
    "# ---------------------------------------------------\n",
    "drop_cols = {target_col, \"target_raw\", \"target_clipped\"}\n",
    "if id_col: drop_cols.add(id_col)\n",
    "\n",
    "feature_cols = [c for c in train.columns if c not in drop_cols and c in test.columns]\n",
    "if not feature_cols:\n",
    "    raise ValueError(\"No common feature columns between train and test after dropping targets/ID.\")\n",
    "\n",
    "# Optional sampling for speed\n",
    "if FAST_DEBUG and len(train) > N_ROWS_DEBUG:\n",
    "    train = train.sample(N_ROWS_DEBUG, random_state=42).reset_index(drop=True)\n",
    "    print(f\"FAST_DEBUG: sampled {len(train):,} rows\")\n",
    "\n",
    "X      = train[feature_cols].copy()\n",
    "X_test = test[feature_cols].copy()\n",
    "y      = train[target_col].values\n",
    "\n",
    "# Dtype coercion: object -> numeric or category\n",
    "for col in X.select_dtypes(include=\"object\").columns:\n",
    "    try:\n",
    "        X[col]      = pd.to_numeric(X[col])\n",
    "        X_test[col] = pd.to_numeric(X_test[col])\n",
    "    except Exception:\n",
    "        X[col]      = X[col].astype(\"category\")\n",
    "        X_test[col] = X_test[col].astype(\"category\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "print(\"Split -> X_tr:\", X_tr.shape, \" | X_val:\", X_val.shape)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# (E) Metrics helper (MAE/RMSE in original units)\n",
    "# ------------------------------------------------\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def eval_scores(y_true_log, y_pred_log, use_log=True):\n",
    "    if use_log:\n",
    "        y_true = np.expm1(y_true_log)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "    else:\n",
    "        y_true = y_true_log\n",
    "        y_pred = y_pred_log\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred) ** 0.5\n",
    "\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse}\n",
    "\n",
    "scores = {}\n",
    "models  = {}\n",
    "\n",
    "# --------------------------------------\n",
    "# (F1) Random Forest — baseline\n",
    "# --------------------------------------\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "t0 = time.time()\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=150 if FAST_DEBUG else 300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "rf.fit(X_tr, y_tr)\n",
    "rf_pred_val = rf.predict(X_val)\n",
    "scores[\"RandomForest\"] = eval_scores(y_val, rf_pred_val, use_log=use_log_target)\n",
    "models[\"RandomForest\"] = rf\n",
    "print(f\"RF: {scores['RandomForest']} | time={time.time()-t0:.1f}s\")\n",
    "\n",
    "# --------------------------------------\n",
    "# (F2) LightGBM — gradient boosting\n",
    "# --------------------------------------\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception:\n",
    "    print(\"Installing lightgbm...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install -q lightgbm\n",
    "    import lightgbm as lgb\n",
    "\n",
    "t0 = time.time()\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    n_estimators=800 if FAST_DEBUG else 2000,\n",
    "    learning_rate=0.10 if FAST_DEBUG else 0.05,\n",
    "    num_leaves=48 if FAST_DEBUG else 64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgbm.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"l2\",\n",
    "    callbacks=[lgb.early_stopping(50 if FAST_DEBUG else 100),\n",
    "               lgb.log_evaluation(100 if FAST_DEBUG else 200)]\n",
    ")\n",
    "lgbm_pred_val = lgbm.predict(X_val, num_iteration=lgbm.best_iteration_)\n",
    "scores[\"LightGBM\"] = eval_scores(y_val, lgbm_pred_val, use_log=use_log_target)\n",
    "models[\"LightGBM\"] = lgbm\n",
    "print(f\"LGBM: {scores['LightGBM']} | best_iter={lgbm.best_iteration_} | time={time.time()-t0:.1f}s\")\n",
    "\n",
    "# --------------------------------------\n",
    "# (F3) XGBoost — native API (xgb.train)\n",
    "# --------------------------------------\n",
    "import xgboost as xgb\n",
    "\n",
    "# Convert splits to DMatrix\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"eta\": 0.10 if FAST_DEBUG else 0.05,  # learning rate\n",
    "    \"max_depth\": 8,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"lambda\": 1.0,                         # L2 reg\n",
    "    \"tree_method\": \"hist\",                 # \"gpu_hist\" if you have a GPU\n",
    "}\n",
    "\n",
    "num_round = 800 if FAST_DEBUG else 2000\n",
    "watchlist = [(dtrain, \"train\"), (dval, \"eval\")]\n",
    "\n",
    "t0 = time.time()\n",
    "bst = xgb.train(\n",
    "    params=xgb_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=num_round,\n",
    "    evals=watchlist,\n",
    "    early_stopping_rounds=50 if FAST_DEBUG else 100,\n",
    "    verbose_eval=100\n",
    ")\n",
    "best_iteration = bst.attributes().get('best_iteration')\n",
    "xgb_pred_val = bst.predict(dval, iteration_range=(0, int(best_iteration) if best_iteration else 0))\n",
    "scores[\"XGBoost\"] = eval_scores(y_val, xgb_pred_val, use_log=use_log_target)\n",
    "print(f\"XGB: {scores['XGBoost']} | best_iter={best_iteration} | time={(time.time() - t0):.2f}s\")\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# (G) Compare & simple ensemble\n",
    "# --------------------------------------\n",
    "print(\"\\nValidation scores (lower is better):\")\n",
    "for k, v in scores.items():\n",
    "    print(f\"{k:12s} -> MAE: {v['MAE']:.4f} | RMSE: {v['RMSE']:.4f}\")\n",
    "\n",
    "# Simple ensemble (avg LGBM + XGB) — often best\n",
    "if \"LightGBM\" in scores and \"XGBoost\" in scores:\n",
    "    ens_pred_val = (lgbm_pred_val + xgb_pred_val) / 2.0\n",
    "    scores[\"Ensemble_LGBM_XGB\"] = eval_scores(y_val, ens_pred_val, use_log=use_log_target)\n",
    "    print(f\"Ensemble     -> MAE: {scores['Ensemble_LGBM_XGB']['MAE']:.4f} | RMSE: {scores['Ensemble_LGBM_XGB']['RMSE']:.4f}\")\n",
    "\n",
    "best_name = min(scores, key=lambda k: scores[k][\"RMSE\"])\n",
    "print(f\"\\n🏆 Best model by RMSE: {best_name}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# (H) Refit best on FULL train and predict on test\n",
    "# ------------------------------------------------------\n",
    "def refit_and_predict(name):\n",
    "    if name == \"RandomForest\":\n",
    "        m = RandomForestRegressor(\n",
    "            n_estimators=150 if FAST_DEBUG else 300,\n",
    "            max_depth=None, min_samples_leaf=2, n_jobs=-1, random_state=42\n",
    "        ).fit(X, y)\n",
    "        return m.predict(X_test)\n",
    "\n",
    "    if name == \"LightGBM\":\n",
    "        m = lgb.LGBMRegressor(\n",
    "            n_estimators=int(lgbm.best_iteration_ or (800 if FAST_DEBUG else 2000)),\n",
    "            learning_rate=0.10 if FAST_DEBUG else 0.05,\n",
    "            num_leaves=48 if FAST_DEBUG else 64,\n",
    "            subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1\n",
    "        ).fit(X, y)\n",
    "        return m.predict(X_test)\n",
    "\n",
    "    if name == \"XGBoost\":\n",
    "        # Refit using native API with best iteration from validation\n",
    "        dtrain_full = xgb.DMatrix(X, label=y)\n",
    "        dtest       = xgb.DMatrix(X_test)\n",
    "        best_iter   = int(getattr(bst, \"best_iteration\", 800 if FAST_DEBUG else 2000))\n",
    "\n",
    "        bst_full = xgb.train(\n",
    "            params=xgb_params,             # reuse same params\n",
    "            dtrain=dtrain_full,\n",
    "            num_boost_round=best_iter,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        return bst_full.predict(dtest)\n",
    "\n",
    "    if name == \"Ensemble_LGBM_XGB\":\n",
    "        # Fit both on full data and average predictions\n",
    "        m1 = lgb.LGBMRegressor(\n",
    "            n_estimators=int(lgbm.best_iteration_ or (800 if FAST_DEBUG else 2000)),\n",
    "            learning_rate=0.10 if FAST_DEBUG else 0.05,\n",
    "            num_leaves=48 if FAST_DEBUG else 64,\n",
    "            subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1\n",
    "        ).fit(X, y)\n",
    "\n",
    "        dtrain_full = xgb.DMatrix(X, label=y)\n",
    "        dtest       = xgb.DMatrix(X_test)\n",
    "        best_iter   = int(getattr(bst, \"best_iteration\", 800 if FAST_DEBUG else 2000))\n",
    "        bst_full    = xgb.train(params=xgb_params, dtrain=dtrain_full, num_boost_round=best_iter, verbose_eval=False)\n",
    "\n",
    "        return (m1.predict(X_test) + bst_full.predict(dtest)) / 2.0\n",
    "\n",
    "    raise ValueError(\"Unknown model name.\")\n",
    "\n",
    "pred_test = refit_and_predict(best_name)\n",
    "\n",
    "# Inverse-transform if trained on log target\n",
    "if use_log_target:\n",
    "    pred_test = np.expm1(pred_test)\n",
    "\n",
    "# Safety clip\n",
    "pred_test = np.clip(pred_test, a_min=1e-6, a_max=None)\n",
    "\n",
    "# -------------------------------------------\n",
    "# (I) Build submission and save\n",
    "# -------------------------------------------\n",
    "if os.path.exists(sample_path):\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    tgt_cols  = [c for c in sample.columns if c.lower() != \"id\"]\n",
    "    sub_tgt   = tgt_cols[0] if tgt_cols else \"trip_duration\"\n",
    "    if id_col and id_col in test.columns and \"id\" in sample.columns:\n",
    "        out = pd.DataFrame({id_col: test[id_col].values, sub_tgt: pred_test})\n",
    "        sub = sample.drop(columns=[sub_tgt], errors=\"ignore\").merge(out, left_on=\"id\", right_on=id_col, how=\"left\")\n",
    "        sub = sub[[\"id\", sub_tgt]]\n",
    "    else:\n",
    "        sub = pd.DataFrame({\"id\": np.arange(len(pred_test)), sub_tgt: pred_test})\n",
    "else:\n",
    "    sub_tgt = \"trip_duration\"\n",
    "    if id_col and id_col in test.columns:\n",
    "        sub = pd.DataFrame({id_col: test[id_col].values, sub_tgt: pred_test}).rename(columns={id_col: \"id\"})\n",
    "    else:\n",
    "        sub = pd.DataFrame({\"id\": np.arange(len(pred_test)), sub_tgt: pred_test})\n",
    "\n",
    "sub_path = os.path.join(BASE, \"submission.csv\")\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print(f\"\\n✅ Done. Saved submission to: {sub_path}\")\n",
    "display(sub.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0749e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search over ensemble weight (validation):\n",
      " w_xgb        MAE       RMSE\n",
      "  0.15 203.671870 336.675282\n",
      "  0.10 203.683481 336.678161\n",
      "  0.20 203.664443 336.678271\n",
      "  0.05 203.700331 336.686916\n",
      "  0.25 203.661532 336.687120\n",
      "  0.00 203.721927 336.701553\n",
      "  0.30 203.663498 336.701823\n",
      "  0.35 203.670169 336.722372\n",
      "\n",
      "🏆 Best weight for XGB on validation: w_xgb=0.15 → MAE=203.67, RMSE=336.68\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1458644, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.462139\n",
      "\n",
      "✅ Ensemble submission saved to: C:\\New folder7\\submission_ensemble.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id3004672</td>\n",
       "      <td>774.742354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id3505355</td>\n",
       "      <td>832.496301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id1217141</td>\n",
       "      <td>440.746450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id2150126</td>\n",
       "      <td>1029.464309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id1598245</td>\n",
       "      <td>337.084155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  trip_duration\n",
       "0  id3004672     774.742354\n",
       "1  id3505355     832.496301\n",
       "2  id1217141     440.746450\n",
       "3  id2150126    1029.464309\n",
       "4  id1598245     337.084155"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ENSEMBLE: XGBoost + LightGBM (validation -> full -> submission)\n",
    "# ==========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# ---- (A) تأكيد المتغيّرات الأساسية موجودة ----\n",
    "needed = [\"lgbm_pred_val\",\"xgb_pred_val\",\"y_val\",\"use_log_target\",\n",
    "          \"lgbm\",\"bst\",\"X\",\"y\",\"X_test\",\"test\",\"id_col\",\"sample_path\",\"BASE\"]\n",
    "missing = [n for n in needed if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Variables missing from session: {missing} — شغّل خلية التدريب أولًا.\")\n",
    "\n",
    "# ---- (B) اختيار وزن المزج على الـ validation ----\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def eval_scores_vec(y_true_log, y_pred_log, use_log=True):\n",
    "    if use_log:\n",
    "        y_true = np.expm1(y_true_log)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "    else:\n",
    "        y_true = y_true_log\n",
    "        y_pred = y_pred_log\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred) ** 0.5\n",
    "    return mae, rmse\n",
    "\n",
    "grid = np.linspace(0.0, 1.0, 21)  # وزن XGB من 0→1 بخطوة 0.05\n",
    "best = {\"w_xgb\": None, \"MAE\": 1e18, \"RMSE\": 1e18}\n",
    "rows = []\n",
    "for w in grid:\n",
    "    pred_val = w * xgb_pred_val + (1 - w) * lgbm_pred_val\n",
    "    mae, rmse = eval_scores_vec(y_val, pred_val, use_log=use_log_target)\n",
    "    rows.append((w, mae, rmse))\n",
    "    if rmse < best[\"RMSE\"]:\n",
    "        best = {\"w_xgb\": float(w), \"MAE\": float(mae), \"RMSE\": float(rmse)}\n",
    "\n",
    "res_df = pd.DataFrame(rows, columns=[\"w_xgb\",\"MAE\",\"RMSE\"]).sort_values(\"RMSE\")\n",
    "print(\"Grid search over ensemble weight (validation):\")\n",
    "print(res_df.head(8).to_string(index=False))\n",
    "print(f\"\\n🏆 Best weight for XGB on validation: w_xgb={best['w_xgb']:.2f} \"\n",
    "      f\"→ MAE={best['MAE']:.2f}, RMSE={best['RMSE']:.2f}\")\n",
    "\n",
    "# ---- (C) إعادة تدريب النموذجين على كامل البيانات ----\n",
    "# LightGBM full\n",
    "lgb_n_estimators = int(getattr(lgbm, \"best_iteration_\", 0) or lgbm.get_params().get(\"n_estimators\", 2000))\n",
    "lgb_full = lgb.LGBMRegressor(**{\n",
    "    **{k:v for k,v in lgbm.get_params().items() if k in\n",
    "       [\"learning_rate\",\"num_leaves\",\"subsample\",\"colsample_bytree\",\"random_state\",\"n_jobs\"]},\n",
    "    \"n_estimators\": lgb_n_estimators\n",
    "})\n",
    "lgb_full.fit(X, y)\n",
    "pred_lgb_test = lgb_full.predict(X_test, num_iteration=getattr(lgb_full, \"best_iteration_\", lgb_n_estimators))\n",
    "\n",
    "# XGBoost full (native API) بنفس إعدادات bst وأفضل تكرار\n",
    "FAST_DEBUG = globals().get(\"FAST_DEBUG\", False)\n",
    "eta = 0.10 if FAST_DEBUG else 0.05\n",
    "xgb_params_full = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"eta\": eta,\n",
    "    \"max_depth\": 8,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"lambda\": 1.0,\n",
    "    \"tree_method\": \"hist\",  # غيّرها لـ \"gpu_hist\" لو عندك GPU\n",
    "}\n",
    "best_iter_attr = (bst.attributes().get(\"best_iteration\") if hasattr(bst, \"attributes\") else None)\n",
    "xgb_best_rounds = int(best_iter_attr) if best_iter_attr is not None else (800 if FAST_DEBUG else 2000)\n",
    "\n",
    "dtrain_full = xgb.DMatrix(X, label=y)\n",
    "dtest       = xgb.DMatrix(X_test)\n",
    "bst_full = xgb.train(params=xgb_params_full, dtrain=dtrain_full, num_boost_round=xgb_best_rounds, verbose_eval=False)\n",
    "pred_xgb_test = bst_full.predict(dtest)  # يستخدم كل الجولات المدربة\n",
    "\n",
    "# ---- (D) المزج بالوزن الأفضل من الـ validation ----\n",
    "w = best[\"w_xgb\"]\n",
    "pred_test_ens = w * pred_xgb_test + (1 - w) * pred_lgb_test\n",
    "\n",
    "# عكس التحويل log1p إن وجد\n",
    "if use_log_target:\n",
    "    pred_test_ens = np.expm1(pred_test_ens)\n",
    "\n",
    "pred_test_ens = np.clip(pred_test_ens, 1e-6, None)\n",
    "\n",
    "# ---- (E) إنشاء ملف التسليم submission.csv ----\n",
    "import os\n",
    "if os.path.exists(sample_path):\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    tgt_cols = [c for c in sample.columns if c.lower() != \"id\"]\n",
    "    sub_tgt  = tgt_cols[0] if tgt_cols else \"trip_duration\"\n",
    "    if id_col and id_col in test.columns and \"id\" in sample.columns:\n",
    "        out = pd.DataFrame({id_col: test[id_col].values, sub_tgt: pred_test_ens})\n",
    "        sub = sample.drop(columns=[sub_tgt], errors=\"ignore\").merge(out, left_on=\"id\", right_on=id_col, how=\"left\")\n",
    "        sub = sub[[\"id\", sub_tgt]]\n",
    "    else:\n",
    "        sub = pd.DataFrame({\"id\": np.arange(len(pred_test_ens)), sub_tgt: pred_test_ens})\n",
    "else:\n",
    "    sub_tgt = \"trip_duration\"\n",
    "    if id_col and id_col in test.columns:\n",
    "        sub = pd.DataFrame({id_col: test[id_col].values, sub_tgt: pred_test_ens}).rename(columns={id_col: \"id\"})\n",
    "    else:\n",
    "        sub = pd.DataFrame({\"id\": np.arange(len(pred_test_ens)), sub_tgt: pred_test_ens})\n",
    "\n",
    "sub_path = os.path.join(BASE, \"submission_ensemble.csv\")\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print(f\"\\n✅ Ensemble submission saved to: {sub_path}\")\n",
    "try:\n",
    "    display(sub.head())\n",
    "except:\n",
    "    print(sub.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2f2259c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Tuning LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.269345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158381\n",
      "[400]\tvalid_0's l2: 0.157898\n",
      "[600]\tvalid_0's l2: 0.157701\n",
      "Early stopping, best iteration is:\n",
      "[568]\tvalid_0's l2: 0.157684\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 20, 'subsample': 0.7, 'colsample_bytree': 0.7} -> RMSE=337.857 | MAE=204.512 | best_iter=568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.125291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158458\n",
      "[400]\tvalid_0's l2: 0.157854\n",
      "[600]\tvalid_0's l2: 0.157776\n",
      "[800]\tvalid_0's l2: 0.157783\n",
      "Early stopping, best iteration is:\n",
      "[653]\tvalid_0's l2: 0.157748\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 20, 'subsample': 0.7, 'colsample_bytree': 0.8} -> RMSE=337.310 | MAE=204.155 | best_iter=653\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.124554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158478\n",
      "[400]\tvalid_0's l2: 0.157843\n",
      "[600]\tvalid_0's l2: 0.15773\n",
      "Early stopping, best iteration is:\n",
      "[585]\tvalid_0's l2: 0.157715\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 20, 'subsample': 0.7, 'colsample_bytree': 0.9} -> RMSE=337.323 | MAE=204.140 | best_iter=585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185008 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158381\n",
      "[400]\tvalid_0's l2: 0.157898\n",
      "[600]\tvalid_0's l2: 0.157701\n",
      "Early stopping, best iteration is:\n",
      "[568]\tvalid_0's l2: 0.157684\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.7} -> RMSE=337.857 | MAE=204.512 | best_iter=568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158458\n",
      "[400]\tvalid_0's l2: 0.157854\n",
      "[600]\tvalid_0's l2: 0.157776\n",
      "[800]\tvalid_0's l2: 0.157783\n",
      "Early stopping, best iteration is:\n",
      "[653]\tvalid_0's l2: 0.157748\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.8} -> RMSE=337.310 | MAE=204.155 | best_iter=653\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158478\n",
      "[400]\tvalid_0's l2: 0.157843\n",
      "[600]\tvalid_0's l2: 0.15773\n",
      "Early stopping, best iteration is:\n",
      "[585]\tvalid_0's l2: 0.157715\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.9} -> RMSE=337.323 | MAE=204.140 | best_iter=585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158381\n",
      "[400]\tvalid_0's l2: 0.157898\n",
      "[600]\tvalid_0's l2: 0.157701\n",
      "Early stopping, best iteration is:\n",
      "[568]\tvalid_0's l2: 0.157684\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 20, 'subsample': 0.9, 'colsample_bytree': 0.7} -> RMSE=337.857 | MAE=204.512 | best_iter=568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119781 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158458\n",
      "[400]\tvalid_0's l2: 0.157854\n",
      "[600]\tvalid_0's l2: 0.157776\n",
      "[800]\tvalid_0's l2: 0.157783\n",
      "Early stopping, best iteration is:\n",
      "[653]\tvalid_0's l2: 0.157748\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 20, 'subsample': 0.9, 'colsample_bytree': 0.8} -> RMSE=337.310 | MAE=204.155 | best_iter=653\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158478\n",
      "[400]\tvalid_0's l2: 0.157843\n",
      "[600]\tvalid_0's l2: 0.15773\n",
      "Early stopping, best iteration is:\n",
      "[585]\tvalid_0's l2: 0.157715\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 20, 'subsample': 0.9, 'colsample_bytree': 0.9} -> RMSE=337.323 | MAE=204.140 | best_iter=585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158387\n",
      "[400]\tvalid_0's l2: 0.157603\n",
      "[600]\tvalid_0's l2: 0.1574\n",
      "[800]\tvalid_0's l2: 0.157338\n",
      "[1000]\tvalid_0's l2: 0.157373\n",
      "Early stopping, best iteration is:\n",
      "[919]\tvalid_0's l2: 0.157326\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 40, 'subsample': 0.7, 'colsample_bytree': 0.7} -> RMSE=336.453 | MAE=203.568 | best_iter=919\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158382\n",
      "[400]\tvalid_0's l2: 0.157676\n",
      "[600]\tvalid_0's l2: 0.157598\n",
      "[800]\tvalid_0's l2: 0.15755\n",
      "[1000]\tvalid_0's l2: 0.157542\n",
      "Early stopping, best iteration is:\n",
      "[943]\tvalid_0's l2: 0.15751\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 40, 'subsample': 0.7, 'colsample_bytree': 0.8} -> RMSE=336.367 | MAE=203.523 | best_iter=943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158344\n",
      "[400]\tvalid_0's l2: 0.157714\n",
      "[600]\tvalid_0's l2: 0.157637\n",
      "[800]\tvalid_0's l2: 0.157581\n",
      "[1000]\tvalid_0's l2: 0.157627\n",
      "Early stopping, best iteration is:\n",
      "[888]\tvalid_0's l2: 0.157531\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 40, 'subsample': 0.7, 'colsample_bytree': 0.9} -> RMSE=336.338 | MAE=203.487 | best_iter=888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.146727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158387\n",
      "[400]\tvalid_0's l2: 0.157603\n",
      "[600]\tvalid_0's l2: 0.1574\n",
      "[800]\tvalid_0's l2: 0.157338\n",
      "[1000]\tvalid_0's l2: 0.157373\n",
      "Early stopping, best iteration is:\n",
      "[919]\tvalid_0's l2: 0.157326\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 40, 'subsample': 0.8, 'colsample_bytree': 0.7} -> RMSE=336.453 | MAE=203.568 | best_iter=919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158382\n",
      "[400]\tvalid_0's l2: 0.157676\n",
      "[600]\tvalid_0's l2: 0.157598\n",
      "[800]\tvalid_0's l2: 0.15755\n",
      "[1000]\tvalid_0's l2: 0.157542\n",
      "Early stopping, best iteration is:\n",
      "[943]\tvalid_0's l2: 0.15751\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 40, 'subsample': 0.8, 'colsample_bytree': 0.8} -> RMSE=336.367 | MAE=203.523 | best_iter=943\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158344\n",
      "[400]\tvalid_0's l2: 0.157714\n",
      "[600]\tvalid_0's l2: 0.157637\n",
      "[800]\tvalid_0's l2: 0.157581\n",
      "[1000]\tvalid_0's l2: 0.157627\n",
      "Early stopping, best iteration is:\n",
      "[888]\tvalid_0's l2: 0.157531\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 40, 'subsample': 0.8, 'colsample_bytree': 0.9} -> RMSE=336.338 | MAE=203.487 | best_iter=888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158387\n",
      "[400]\tvalid_0's l2: 0.157603\n",
      "[600]\tvalid_0's l2: 0.1574\n",
      "[800]\tvalid_0's l2: 0.157338\n",
      "[1000]\tvalid_0's l2: 0.157373\n",
      "Early stopping, best iteration is:\n",
      "[919]\tvalid_0's l2: 0.157326\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 40, 'subsample': 0.9, 'colsample_bytree': 0.7} -> RMSE=336.453 | MAE=203.568 | best_iter=919\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.150639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158382\n",
      "[400]\tvalid_0's l2: 0.157676\n",
      "[600]\tvalid_0's l2: 0.157598\n",
      "[800]\tvalid_0's l2: 0.15755\n",
      "[1000]\tvalid_0's l2: 0.157542\n",
      "Early stopping, best iteration is:\n",
      "[943]\tvalid_0's l2: 0.15751\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 40, 'subsample': 0.9, 'colsample_bytree': 0.8} -> RMSE=336.367 | MAE=203.523 | best_iter=943\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158344\n",
      "[400]\tvalid_0's l2: 0.157714\n",
      "[600]\tvalid_0's l2: 0.157637\n",
      "[800]\tvalid_0's l2: 0.157581\n",
      "[1000]\tvalid_0's l2: 0.157627\n",
      "Early stopping, best iteration is:\n",
      "[888]\tvalid_0's l2: 0.157531\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 40, 'subsample': 0.9, 'colsample_bytree': 0.9} -> RMSE=336.338 | MAE=203.487 | best_iter=888\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158331\n",
      "[400]\tvalid_0's l2: 0.157524\n",
      "[600]\tvalid_0's l2: 0.157231\n",
      "[800]\tvalid_0's l2: 0.15728\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's l2: 0.157184\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 80, 'subsample': 0.7, 'colsample_bytree': 0.7} -> RMSE=336.902 | MAE=203.838 | best_iter=662\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158244\n",
      "[400]\tvalid_0's l2: 0.157427\n",
      "[600]\tvalid_0's l2: 0.157241\n",
      "[800]\tvalid_0's l2: 0.157211\n",
      "Early stopping, best iteration is:\n",
      "[783]\tvalid_0's l2: 0.157168\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 80, 'subsample': 0.7, 'colsample_bytree': 0.8} -> RMSE=336.411 | MAE=203.529 | best_iter=783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158232\n",
      "[400]\tvalid_0's l2: 0.157495\n",
      "[600]\tvalid_0's l2: 0.157303\n",
      "[800]\tvalid_0's l2: 0.157205\n",
      "Early stopping, best iteration is:\n",
      "[779]\tvalid_0's l2: 0.15719\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 80, 'subsample': 0.7, 'colsample_bytree': 0.9} -> RMSE=336.301 | MAE=203.441 | best_iter=779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158331\n",
      "[400]\tvalid_0's l2: 0.157524\n",
      "[600]\tvalid_0's l2: 0.157231\n",
      "[800]\tvalid_0's l2: 0.15728\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's l2: 0.157184\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 80, 'subsample': 0.8, 'colsample_bytree': 0.7} -> RMSE=336.902 | MAE=203.838 | best_iter=662\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158244\n",
      "[400]\tvalid_0's l2: 0.157427\n",
      "[600]\tvalid_0's l2: 0.157241\n",
      "[800]\tvalid_0's l2: 0.157211\n",
      "Early stopping, best iteration is:\n",
      "[783]\tvalid_0's l2: 0.157168\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 80, 'subsample': 0.8, 'colsample_bytree': 0.8} -> RMSE=336.411 | MAE=203.529 | best_iter=783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158232\n",
      "[400]\tvalid_0's l2: 0.157495\n",
      "[600]\tvalid_0's l2: 0.157303\n",
      "[800]\tvalid_0's l2: 0.157205\n",
      "Early stopping, best iteration is:\n",
      "[779]\tvalid_0's l2: 0.15719\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 80, 'subsample': 0.8, 'colsample_bytree': 0.9} -> RMSE=336.301 | MAE=203.441 | best_iter=779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158331\n",
      "[400]\tvalid_0's l2: 0.157524\n",
      "[600]\tvalid_0's l2: 0.157231\n",
      "[800]\tvalid_0's l2: 0.15728\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's l2: 0.157184\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 80, 'subsample': 0.9, 'colsample_bytree': 0.7} -> RMSE=336.902 | MAE=203.838 | best_iter=662\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158244\n",
      "[400]\tvalid_0's l2: 0.157427\n",
      "[600]\tvalid_0's l2: 0.157241\n",
      "[800]\tvalid_0's l2: 0.157211\n",
      "Early stopping, best iteration is:\n",
      "[783]\tvalid_0's l2: 0.157168\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 80, 'subsample': 0.9, 'colsample_bytree': 0.8} -> RMSE=336.411 | MAE=203.529 | best_iter=783\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.139646 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158232\n",
      "[400]\tvalid_0's l2: 0.157495\n",
      "[600]\tvalid_0's l2: 0.157303\n",
      "[800]\tvalid_0's l2: 0.157205\n",
      "Early stopping, best iteration is:\n",
      "[779]\tvalid_0's l2: 0.15719\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 48, 'min_child_samples': 80, 'subsample': 0.9, 'colsample_bytree': 0.9} -> RMSE=336.301 | MAE=203.441 | best_iter=779\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157982\n",
      "[400]\tvalid_0's l2: 0.157601\n",
      "[600]\tvalid_0's l2: 0.157682\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's l2: 0.157571\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 20, 'subsample': 0.7, 'colsample_bytree': 0.7} -> RMSE=337.776 | MAE=204.459 | best_iter=429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158182\n",
      "[400]\tvalid_0's l2: 0.157772\n",
      "[600]\tvalid_0's l2: 0.157736\n",
      "[800]\tvalid_0's l2: 0.157745\n",
      "Early stopping, best iteration is:\n",
      "[678]\tvalid_0's l2: 0.157686\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 20, 'subsample': 0.7, 'colsample_bytree': 0.8} -> RMSE=336.702 | MAE=203.722 | best_iter=678\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.141243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158151\n",
      "[400]\tvalid_0's l2: 0.157857\n",
      "[600]\tvalid_0's l2: 0.157885\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's l2: 0.157827\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 20, 'subsample': 0.7, 'colsample_bytree': 0.9} -> RMSE=337.574 | MAE=204.331 | best_iter=429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092636 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157982\n",
      "[400]\tvalid_0's l2: 0.157601\n",
      "[600]\tvalid_0's l2: 0.157682\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's l2: 0.157571\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.7} -> RMSE=337.776 | MAE=204.459 | best_iter=429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158182\n",
      "[400]\tvalid_0's l2: 0.157772\n",
      "[600]\tvalid_0's l2: 0.157736\n",
      "[800]\tvalid_0's l2: 0.157745\n",
      "Early stopping, best iteration is:\n",
      "[678]\tvalid_0's l2: 0.157686\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.8} -> RMSE=336.702 | MAE=203.722 | best_iter=678\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.142668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158151\n",
      "[400]\tvalid_0's l2: 0.157857\n",
      "[600]\tvalid_0's l2: 0.157885\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's l2: 0.157827\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.9} -> RMSE=337.574 | MAE=204.331 | best_iter=429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.135533 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157982\n",
      "[400]\tvalid_0's l2: 0.157601\n",
      "[600]\tvalid_0's l2: 0.157682\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's l2: 0.157571\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 20, 'subsample': 0.9, 'colsample_bytree': 0.7} -> RMSE=337.776 | MAE=204.459 | best_iter=429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.144056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158182\n",
      "[400]\tvalid_0's l2: 0.157772\n",
      "[600]\tvalid_0's l2: 0.157736\n",
      "[800]\tvalid_0's l2: 0.157745\n",
      "Early stopping, best iteration is:\n",
      "[678]\tvalid_0's l2: 0.157686\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 20, 'subsample': 0.9, 'colsample_bytree': 0.8} -> RMSE=336.702 | MAE=203.722 | best_iter=678\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.158151\n",
      "[400]\tvalid_0's l2: 0.157857\n",
      "[600]\tvalid_0's l2: 0.157885\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's l2: 0.157827\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 20, 'subsample': 0.9, 'colsample_bytree': 0.9} -> RMSE=337.574 | MAE=204.331 | best_iter=429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157947\n",
      "[400]\tvalid_0's l2: 0.157478\n",
      "[600]\tvalid_0's l2: 0.157417\n",
      "Early stopping, best iteration is:\n",
      "[593]\tvalid_0's l2: 0.157407\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 40, 'subsample': 0.7, 'colsample_bytree': 0.7} -> RMSE=336.989 | MAE=203.925 | best_iter=593\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157944\n",
      "[400]\tvalid_0's l2: 0.157474\n",
      "[600]\tvalid_0's l2: 0.157449\n",
      "Early stopping, best iteration is:\n",
      "[454]\tvalid_0's l2: 0.157434\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 40, 'subsample': 0.7, 'colsample_bytree': 0.8} -> RMSE=337.270 | MAE=204.096 | best_iter=454\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157962\n",
      "[400]\tvalid_0's l2: 0.157502\n",
      "[600]\tvalid_0's l2: 0.157411\n",
      "[800]\tvalid_0's l2: 0.15749\n",
      "Early stopping, best iteration is:\n",
      "[609]\tvalid_0's l2: 0.157398\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 40, 'subsample': 0.7, 'colsample_bytree': 0.9} -> RMSE=336.651 | MAE=203.704 | best_iter=609\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157947\n",
      "[400]\tvalid_0's l2: 0.157478\n",
      "[600]\tvalid_0's l2: 0.157417\n",
      "Early stopping, best iteration is:\n",
      "[593]\tvalid_0's l2: 0.157407\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 40, 'subsample': 0.8, 'colsample_bytree': 0.7} -> RMSE=336.989 | MAE=203.925 | best_iter=593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.100641 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157944\n",
      "[400]\tvalid_0's l2: 0.157474\n",
      "[600]\tvalid_0's l2: 0.157449\n",
      "Early stopping, best iteration is:\n",
      "[454]\tvalid_0's l2: 0.157434\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 40, 'subsample': 0.8, 'colsample_bytree': 0.8} -> RMSE=337.270 | MAE=204.096 | best_iter=454\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108688 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157962\n",
      "[400]\tvalid_0's l2: 0.157502\n",
      "[600]\tvalid_0's l2: 0.157411\n",
      "[800]\tvalid_0's l2: 0.15749\n",
      "Early stopping, best iteration is:\n",
      "[609]\tvalid_0's l2: 0.157398\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 40, 'subsample': 0.8, 'colsample_bytree': 0.9} -> RMSE=336.651 | MAE=203.704 | best_iter=609\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157947\n",
      "[400]\tvalid_0's l2: 0.157478\n",
      "[600]\tvalid_0's l2: 0.157417\n",
      "Early stopping, best iteration is:\n",
      "[593]\tvalid_0's l2: 0.157407\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 40, 'subsample': 0.9, 'colsample_bytree': 0.7} -> RMSE=336.989 | MAE=203.925 | best_iter=593\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.115299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157944\n",
      "[400]\tvalid_0's l2: 0.157474\n",
      "[600]\tvalid_0's l2: 0.157449\n",
      "Early stopping, best iteration is:\n",
      "[454]\tvalid_0's l2: 0.157434\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 40, 'subsample': 0.9, 'colsample_bytree': 0.8} -> RMSE=337.270 | MAE=204.096 | best_iter=454\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157962\n",
      "[400]\tvalid_0's l2: 0.157502\n",
      "[600]\tvalid_0's l2: 0.157411\n",
      "[800]\tvalid_0's l2: 0.15749\n",
      "Early stopping, best iteration is:\n",
      "[609]\tvalid_0's l2: 0.157398\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 40, 'subsample': 0.9, 'colsample_bytree': 0.9} -> RMSE=336.651 | MAE=203.704 | best_iter=609\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157844\n",
      "[400]\tvalid_0's l2: 0.157344\n",
      "[600]\tvalid_0's l2: 0.157237\n",
      "Early stopping, best iteration is:\n",
      "[583]\tvalid_0's l2: 0.157229\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 80, 'subsample': 0.7, 'colsample_bytree': 0.7} -> RMSE=336.523 | MAE=203.580 | best_iter=583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088886 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157892\n",
      "[400]\tvalid_0's l2: 0.157313\n",
      "[600]\tvalid_0's l2: 0.157288\n",
      "Early stopping, best iteration is:\n",
      "[587]\tvalid_0's l2: 0.15727\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 80, 'subsample': 0.7, 'colsample_bytree': 0.8} -> RMSE=336.470 | MAE=203.553 | best_iter=587\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.112382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157923\n",
      "[400]\tvalid_0's l2: 0.157247\n",
      "[600]\tvalid_0's l2: 0.157117\n",
      "[800]\tvalid_0's l2: 0.157184\n",
      "Early stopping, best iteration is:\n",
      "[691]\tvalid_0's l2: 0.157072\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 80, 'subsample': 0.7, 'colsample_bytree': 0.9} -> RMSE=336.128 | MAE=203.327 | best_iter=691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157844\n",
      "[400]\tvalid_0's l2: 0.157344\n",
      "[600]\tvalid_0's l2: 0.157237\n",
      "Early stopping, best iteration is:\n",
      "[583]\tvalid_0's l2: 0.157229\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 80, 'subsample': 0.8, 'colsample_bytree': 0.7} -> RMSE=336.523 | MAE=203.580 | best_iter=583\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157892\n",
      "[400]\tvalid_0's l2: 0.157313\n",
      "[600]\tvalid_0's l2: 0.157288\n",
      "Early stopping, best iteration is:\n",
      "[587]\tvalid_0's l2: 0.15727\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 80, 'subsample': 0.8, 'colsample_bytree': 0.8} -> RMSE=336.470 | MAE=203.553 | best_iter=587\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157923\n",
      "[400]\tvalid_0's l2: 0.157247\n",
      "[600]\tvalid_0's l2: 0.157117\n",
      "[800]\tvalid_0's l2: 0.157184\n",
      "Early stopping, best iteration is:\n",
      "[691]\tvalid_0's l2: 0.157072\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 80, 'subsample': 0.8, 'colsample_bytree': 0.9} -> RMSE=336.128 | MAE=203.327 | best_iter=691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.136749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157844\n",
      "[400]\tvalid_0's l2: 0.157344\n",
      "[600]\tvalid_0's l2: 0.157237\n",
      "Early stopping, best iteration is:\n",
      "[583]\tvalid_0's l2: 0.157229\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 80, 'subsample': 0.9, 'colsample_bytree': 0.7} -> RMSE=336.523 | MAE=203.580 | best_iter=583\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157892\n",
      "[400]\tvalid_0's l2: 0.157313\n",
      "[600]\tvalid_0's l2: 0.157288\n",
      "Early stopping, best iteration is:\n",
      "[587]\tvalid_0's l2: 0.15727\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 80, 'subsample': 0.9, 'colsample_bytree': 0.8} -> RMSE=336.470 | MAE=203.553 | best_iter=587\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157923\n",
      "[400]\tvalid_0's l2: 0.157247\n",
      "[600]\tvalid_0's l2: 0.157117\n",
      "[800]\tvalid_0's l2: 0.157184\n",
      "Early stopping, best iteration is:\n",
      "[691]\tvalid_0's l2: 0.157072\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 64, 'min_child_samples': 80, 'subsample': 0.9, 'colsample_bytree': 0.9} -> RMSE=336.128 | MAE=203.327 | best_iter=691\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157714\n",
      "[400]\tvalid_0's l2: 0.157671\n",
      "Early stopping, best iteration is:\n",
      "[308]\tvalid_0's l2: 0.157608\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 20, 'subsample': 0.7, 'colsample_bytree': 0.7} -> RMSE=338.035 | MAE=204.551 | best_iter=308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078900 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157803\n",
      "[400]\tvalid_0's l2: 0.157721\n",
      "Early stopping, best iteration is:\n",
      "[332]\tvalid_0's l2: 0.157682\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 20, 'subsample': 0.7, 'colsample_bytree': 0.8} -> RMSE=337.609 | MAE=204.345 | best_iter=332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157787\n",
      "[400]\tvalid_0's l2: 0.157583\n",
      "[600]\tvalid_0's l2: 0.157699\n",
      "Early stopping, best iteration is:\n",
      "[437]\tvalid_0's l2: 0.157566\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 20, 'subsample': 0.7, 'colsample_bytree': 0.9} -> RMSE=336.774 | MAE=203.793 | best_iter=437\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093501 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157714\n",
      "[400]\tvalid_0's l2: 0.157671\n",
      "Early stopping, best iteration is:\n",
      "[308]\tvalid_0's l2: 0.157608\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.7} -> RMSE=338.035 | MAE=204.551 | best_iter=308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157803\n",
      "[400]\tvalid_0's l2: 0.157721\n",
      "Early stopping, best iteration is:\n",
      "[332]\tvalid_0's l2: 0.157682\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.8} -> RMSE=337.609 | MAE=204.345 | best_iter=332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157787\n",
      "[400]\tvalid_0's l2: 0.157583\n",
      "[600]\tvalid_0's l2: 0.157699\n",
      "Early stopping, best iteration is:\n",
      "[437]\tvalid_0's l2: 0.157566\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.9} -> RMSE=336.774 | MAE=203.793 | best_iter=437\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157714\n",
      "[400]\tvalid_0's l2: 0.157671\n",
      "Early stopping, best iteration is:\n",
      "[308]\tvalid_0's l2: 0.157608\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 20, 'subsample': 0.9, 'colsample_bytree': 0.7} -> RMSE=338.035 | MAE=204.551 | best_iter=308\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157803\n",
      "[400]\tvalid_0's l2: 0.157721\n",
      "Early stopping, best iteration is:\n",
      "[332]\tvalid_0's l2: 0.157682\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 20, 'subsample': 0.9, 'colsample_bytree': 0.8} -> RMSE=337.609 | MAE=204.345 | best_iter=332\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.138802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157787\n",
      "[400]\tvalid_0's l2: 0.157583\n",
      "[600]\tvalid_0's l2: 0.157699\n",
      "Early stopping, best iteration is:\n",
      "[437]\tvalid_0's l2: 0.157566\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 20, 'subsample': 0.9, 'colsample_bytree': 0.9} -> RMSE=336.774 | MAE=203.793 | best_iter=437\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157601\n",
      "[400]\tvalid_0's l2: 0.157515\n",
      "[600]\tvalid_0's l2: 0.157561\n",
      "Early stopping, best iteration is:\n",
      "[466]\tvalid_0's l2: 0.157466\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 40, 'subsample': 0.7, 'colsample_bytree': 0.7} -> RMSE=336.869 | MAE=203.827 | best_iter=466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157668\n",
      "[400]\tvalid_0's l2: 0.15738\n",
      "[600]\tvalid_0's l2: 0.157418\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's l2: 0.157368\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 40, 'subsample': 0.7, 'colsample_bytree': 0.8} -> RMSE=336.645 | MAE=203.702 | best_iter=429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093540 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157548\n",
      "[400]\tvalid_0's l2: 0.157283\n",
      "[600]\tvalid_0's l2: 0.157362\n",
      "Early stopping, best iteration is:\n",
      "[520]\tvalid_0's l2: 0.15727\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 40, 'subsample': 0.7, 'colsample_bytree': 0.9} -> RMSE=336.138 | MAE=203.401 | best_iter=520\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157601\n",
      "[400]\tvalid_0's l2: 0.157515\n",
      "[600]\tvalid_0's l2: 0.157561\n",
      "Early stopping, best iteration is:\n",
      "[466]\tvalid_0's l2: 0.157466\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 40, 'subsample': 0.8, 'colsample_bytree': 0.7} -> RMSE=336.869 | MAE=203.827 | best_iter=466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.117098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157668\n",
      "[400]\tvalid_0's l2: 0.15738\n",
      "[600]\tvalid_0's l2: 0.157418\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's l2: 0.157368\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 40, 'subsample': 0.8, 'colsample_bytree': 0.8} -> RMSE=336.645 | MAE=203.702 | best_iter=429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157548\n",
      "[400]\tvalid_0's l2: 0.157283\n",
      "[600]\tvalid_0's l2: 0.157362\n",
      "Early stopping, best iteration is:\n",
      "[520]\tvalid_0's l2: 0.15727\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 40, 'subsample': 0.8, 'colsample_bytree': 0.9} -> RMSE=336.138 | MAE=203.401 | best_iter=520\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157601\n",
      "[400]\tvalid_0's l2: 0.157515\n",
      "[600]\tvalid_0's l2: 0.157561\n",
      "Early stopping, best iteration is:\n",
      "[466]\tvalid_0's l2: 0.157466\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 40, 'subsample': 0.9, 'colsample_bytree': 0.7} -> RMSE=336.869 | MAE=203.827 | best_iter=466\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157668\n",
      "[400]\tvalid_0's l2: 0.15738\n",
      "[600]\tvalid_0's l2: 0.157418\n",
      "Early stopping, best iteration is:\n",
      "[429]\tvalid_0's l2: 0.157368\n",
      "LGBM {'learning_rate': 0.05, 'num_leaves': 96, 'min_child_samples': 40, 'subsample': 0.9, 'colsample_bytree': 0.8} -> RMSE=336.645 | MAE=203.702 | best_iter=429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.461925\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 0.157548\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m vals \u001b[38;5;129;01min\u001b[39;00m itertools.product(*lgb_grid.values()):\n\u001b[32m     48\u001b[39m     params = \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(lgb_grid.keys(), vals))\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     rmse, mae, model = \u001b[43mlgb_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     lgb_results.append((rmse, mae, params, \u001b[38;5;28mint\u001b[39m(model.best_iteration_)))\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLGBM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> RMSE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | MAE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | best_iter=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(model.best_iteration_)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mlgb_eval\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m     28\u001b[39m model = lgb.LGBMRegressor(\n\u001b[32m     29\u001b[39m     n_estimators=\u001b[32m3000\u001b[39m,              \u001b[38;5;66;03m# كبير + إيقاف مبكر\u001b[39;00m\n\u001b[32m     30\u001b[39m     learning_rate=params[\u001b[33m\"\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     37\u001b[39m )\n\u001b[32m     38\u001b[39m callbacks = [lgb.early_stopping(\u001b[32m200\u001b[39m), lgb.log_evaluation(\u001b[32m200\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ml2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m pred_val = model.predict(X_val, num_iteration=model.best_iteration_)\n\u001b[32m     41\u001b[39m rmse, mae = rmse_mae_from_logs(y_val, pred_val, use_log=use_log_target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\sklearn.py:1398\u001b[39m, in \u001b[36mLGBMRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1383\u001b[39m     X: _LGBM_ScikitMatrixLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1395\u001b[39m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1396\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLGBMRegressor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1397\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1398\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Hyperparameter Tuning (LightGBM + XGBoost native) + Ensemble\n",
    "# ==========================================================\n",
    "import time, itertools, numpy as np, pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def rmse_mae_from_logs(y_true_log, y_pred_log, use_log=True):\n",
    "    if use_log:\n",
    "        y_true = np.expm1(y_true_log); y_pred = np.expm1(y_pred_log)\n",
    "    else:\n",
    "        y_true = y_true_log; y_pred = y_pred_log\n",
    "    rmse = mean_squared_error(y_true, y_pred) ** 0.5\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    return rmse, mae\n",
    "\n",
    "# ---------- (A) LightGBM tuning ----------\n",
    "import lightgbm as lgb\n",
    "ش\n",
    "lgb_grid = {\n",
    "    \"learning_rate\": [0.05, 0.075, 0.1],\n",
    "    \"num_leaves\":    [48, 64, 96, 128],\n",
    "    \"min_child_samples\": [20, 40, 80],\n",
    "    \"subsample\":     [0.7, 0.8, 0.9],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "def lgb_eval(params):\n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=3000,              # كبير + إيقاف مبكر\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        num_leaves=params[\"num_leaves\"],\n",
    "        min_child_samples=params[\"min_child_samples\"],\n",
    "        subsample=params[\"subsample\"],\n",
    "        colsample_bytree=params[\"colsample_bytree\"],\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    callbacks = [lgb.early_stopping(200), lgb.log_evaluation(200)]\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric=\"l2\", callbacks=callbacks)\n",
    "    pred_val = model.predict(X_val, num_iteration=model.best_iteration_)\n",
    "    rmse, mae = rmse_mae_from_logs(y_val, pred_val, use_log=use_log_target)\n",
    "    return rmse, mae, model\n",
    "\n",
    "print(\"▶ Tuning LightGBM...\")\n",
    "t0 = time.time()\n",
    "lgb_results = []\n",
    "for vals in itertools.product(*lgb_grid.values()):\n",
    "    params = dict(zip(lgb_grid.keys(), vals))\n",
    "    rmse, mae, model = lgb_eval(params)\n",
    "    lgb_results.append((rmse, mae, params, int(model.best_iteration_)))\n",
    "    print(f\"LGBM {params} -> RMSE={rmse:.3f} | MAE={mae:.3f} | best_iter={int(model.best_iteration_)}\")\n",
    "\n",
    "lgb_results.sort(key=lambda r: r[0])\n",
    "best_lgb_rmse, best_lgb_mae, best_lgb_params, best_lgb_iter = lgb_results[0]\n",
    "print(f\"\\n🏆 Best LGBM: RMSE={best_lgb_rmse:.3f}, MAE={best_lgb_mae:.3f}, params={best_lgb_params}, best_iter={best_lgb_iter}, time={(time.time()-t0):.1f}s\")\n",
    "\n",
    "# ---------- (B) XGBoost tuning (native API: xgb.train) ----------\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_grid = {\n",
    "    \"eta\":        [0.03, 0.05, 0.07, 0.10],  # learning_rate\n",
    "    \"max_depth\":  [6, 7, 8],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"subsample\":  [0.7, 0.8, 0.9],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "    \"lambda\":     [1.0, 1.5, 2.0],\n",
    "}\n",
    "\n",
    "def xgb_eval(params):\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "    p = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"eta\": params[\"eta\"],\n",
    "        \"max_depth\": params[\"max_depth\"],\n",
    "        \"min_child_weight\": params[\"min_child_weight\"],\n",
    "        \"subsample\": params[\"subsample\"],\n",
    "        \"colsample_bytree\": params[\"colsample_bytree\"],\n",
    "        \"lambda\": params[\"lambda\"],\n",
    "        \"tree_method\": \"hist\"\n",
    "    }\n",
    "    watch = [(dtrain,\"train\"), (dval,\"eval\")]\n",
    "    bst = xgb.train(p, dtrain, num_boost_round=4000, evals=watch,\n",
    "                    early_stopping_rounds=200, verbose_eval=False)\n",
    "    best_iter = int(bst.attributes().get(\"best_iteration\") or bst.best_iteration if hasattr(bst,\"best_iteration\") else bst.num_boost_round)\n",
    "    pred_val = bst.predict(dval, iteration_range=(0, best_iter))\n",
    "    rmse, mae = rmse_mae_from_logs(y_val, pred_val, use_log=use_log_target)\n",
    "    return rmse, mae, p, best_iter\n",
    "\n",
    "print(\"\\n▶ Tuning XGBoost...\")\n",
    "t0 = time.time()\n",
    "xgb_results = []\n",
    "# نجرب عدد محدود (عشوائي بسيط) بدل كل التركيبات لتقليل الزمن:\n",
    "import random\n",
    "random.seed(42)\n",
    "all_combos = list(itertools.product(*xgb_grid.values()))\n",
    "random.shuffle(all_combos)\n",
    "sampled = all_combos[:40]   # عدّل الرقم لو تريد تجارب أكثر/أقل\n",
    "\n",
    "for vals in sampled:\n",
    "    params = dict(zip(xgb_grid.keys(), vals))\n",
    "    rmse, mae, p, best_iter = xgb_eval(params)\n",
    "    xgb_results.append((rmse, mae, p, best_iter))\n",
    "    print(f\"XGB {params} -> RMSE={rmse:.3f} | MAE={mae:.3f} | best_iter={best_iter}\")\n",
    "\n",
    "xgb_results.sort(key=lambda r: r[0])\n",
    "best_xgb_rmse, best_xgb_mae, best_xgb_params, best_xgb_iter = xgb_results[0]\n",
    "print(f\"\\n🏆 Best XGB: RMSE={best_xgb_rmse:.3f}, MAE={best_xgb_mae:.3f}, \"\n",
    "      f\"params={best_xgb_params}, best_iter={best_xgb_iter}, time={(time.time()-t0):.1f}s\")\n",
    "\n",
    "# ---------- (C) Refit both on FULL data ----------\n",
    "print(\"\\n▶ Refit best params on FULL train...\")\n",
    "\n",
    "# LightGBM full\n",
    "lgb_full = lgb.LGBMRegressor(\n",
    "    n_estimators=best_lgb_iter,\n",
    "    learning_rate=best_lgb_params[\"learning_rate\"],\n",
    "    num_leaves=best_lgb_params[\"num_leaves\"],\n",
    "    min_child_samples=best_lgb_params[\"min_child_samples\"],\n",
    "    subsample=best_lgb_params[\"subsample\"],\n",
    "    colsample_bytree=best_lgb_params[\"colsample_bytree\"],\n",
    "    random_state=42, n_jobs=-1\n",
    ").fit(X, y)\n",
    "pred_lgb_test = lgb_full.predict(X_test, num_iteration=best_lgb_iter)\n",
    "\n",
    "# XGBoost full\n",
    "dtrain_full = xgb.DMatrix(X, label=y)\n",
    "dtest       = xgb.DMatrix(X_test)\n",
    "bst_full = xgb.train(best_xgb_params, dtrain_full, num_boost_round=best_xgb_iter, verbose_eval=False)\n",
    "pred_xgb_test = bst_full.predict(dtest, iteration_range=(0, best_xgb_iter))\n",
    "\n",
    "# ---------- (D) Find best ensemble weight on validation again ----------\n",
    "# نحسب تنبؤات validation لأفضل إعدادات (للحصول على الوزن الأمثل)\n",
    "# LGBM val\n",
    "lgb_val_model = lgb.LGBMRegressor(\n",
    "    n_estimators=best_lgb_iter,\n",
    "    learning_rate=best_lgb_params[\"learning_rate\"],\n",
    "    num_leaves=best_lgb_params[\"num_leaves\"],\n",
    "    min_child_samples=best_lgb_params[\"min_child_samples\"],\n",
    "    subsample=best_lgb_params[\"subsample\"],\n",
    "    colsample_bytree=best_lgb_params[\"colsample_bytree\"],\n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "lgb_val_model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric=\"l2\", callbacks=[lgb.early_stopping(50)])\n",
    "pred_lgb_val = lgb_val_model.predict(X_val, num_iteration=lgb_val_model.best_iteration_)\n",
    "\n",
    "# XGB val\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "bst_val = xgb.train(best_xgb_params, dtrain, num_boost_round=best_xgb_iter, evals=[(dval,\"eval\")], verbose_eval=False)\n",
    "pred_xgb_val = bst_val.predict(dval, iteration_range=(0, best_xgb_iter))\n",
    "\n",
    "weights = np.linspace(0,1,21)\n",
    "best_w = (None, 1e18, 1e18)\n",
    "for w in weights:\n",
    "    pv = w*pred_xgb_val + (1-w)*pred_lgb_val\n",
    "    rmse, mae = rmse_mae_from_logs(y_val, pv, use_log=use_log_target)\n",
    "    if rmse < best_w[1]:\n",
    "        best_w = (w, rmse, mae)\n",
    "print(f\"\\n🏁 Best ensemble weight on validation after tuning: w_xgb={best_w[0]:.2f} \"\n",
    "      f\"→ RMSE={best_w[1]:.2f}, MAE={best_w[2]:.2f}\")\n",
    "\n",
    "# ---------- (E) Build tuned ensemble predictions on TEST ----------\n",
    "pred_test_ens = best_w[0]*pred_xgb_test + (1-best_w[0])*pred_lgb_test\n",
    "if use_log_target:\n",
    "    pred_test_ens = np.expm1(pred_test_ens)\n",
    "pred_test_ens = np.clip(pred_test_ens, 1e-6, None)\n",
    "\n",
    "# ---------- (F) Save submission ----------\n",
    "import os\n",
    "if os.path.exists(sample_path):\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    tgt_cols = [c for c in sample.columns if c.lower() != \"id\"]\n",
    "    sub_tgt  = tgt_cols[0] if tgt_cols else \"trip_duration\"\n",
    "    if id_col and id_col in test.columns and \"id\" in sample.columns:\n",
    "        out = pd.DataFrame({id_col: test[id_col].values, sub_tgt: pred_test_ens})\n",
    "        sub = sample.drop(columns=[sub_tgt], errors=\"ignore\").merge(out, left_on=\"id\", right_on=id_col, how=\"left\")\n",
    "        sub = sub[[\"id\", sub_tgt]]\n",
    "    else:\n",
    "        sub = pd.DataFrame({\"id\": np.arange(len(pred_test_ens)), sub_tgt: pred_test_ens})\n",
    "else:\n",
    "    sub_tgt = \"trip_duration\"\n",
    "    if id_col and id_col in test.columns:\n",
    "        sub = pd.DataFrame({id_col: test[id_col].values, sub_tgt: pred_test_ens}).rename(columns={id_col: \"id\"})\n",
    "    else:\n",
    "        sub = pd.DataFrame({\"id\": np.arange(len(pred_test_ens)), sub_tgt: pred_test_ens})\n",
    "\n",
    "sub_path = os.path.join(BASE, \"submission_ensemble_tuned.csv\")\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print(f\"\\n✅ Tuned ensemble submission saved to: {sub_path}\")\n",
    "try:\n",
    "    display(sub.head())\n",
    "except:\n",
    "    print(sub.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "997572e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ensemble weights on validation:\n",
      " w_xgb       RMSE        MAE\n",
      "  0.15 336.675282 203.671870\n",
      "  0.10 336.678161 203.683481\n",
      "  0.20 336.678271 203.664443\n",
      "  0.05 336.686916 203.700331\n",
      "  0.25 336.687120 203.661532\n",
      "  0.00 336.701553 203.721927\n",
      "\n",
      "🏆 Best weight: w_xgb=0.15 → RMSE=336.68, MAE=203.67\n",
      "\n",
      "إعادة تدريب LightGBM على كامل البيانات...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.221655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1386\n",
      "[LightGBM] [Info] Number of data points in the train set: 1458644, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 6.462139\n",
      "إعادة تدريب XGBoost (native) على كامل البيانات...\n",
      "\n",
      "✅ Ensemble submission saved to: C:\\New folder7\\submission_ensemble_final.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id3004672</td>\n",
       "      <td>774.742326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id3505355</td>\n",
       "      <td>832.496301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id1217141</td>\n",
       "      <td>440.746434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id2150126</td>\n",
       "      <td>1029.464260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id1598245</td>\n",
       "      <td>337.084151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  trip_duration\n",
       "0  id3004672     774.742326\n",
       "1  id3505355     832.496301\n",
       "2  id1217141     440.746434\n",
       "3  id2150126    1029.464260\n",
       "4  id1598245     337.084151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Final Ensemble (XGBoost + LightGBM) with best weight\n",
    "# ==========================================\n",
    "import numpy as np, pandas as pd, os, time\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- تحقق من المتغيّرات الأساسية ---\n",
    "need = [\"X_tr\",\"X_val\",\"y_tr\",\"y_val\",\"X\",\"y\",\"X_test\",\"test\",\n",
    "        \"id_col\",\"use_log_target\",\"sample_path\",\"BASE\",\"lgbm\",\"bst\",\"xgb_params\"]\n",
    "missing = [n for n in need if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"متغيّرات ناقصة في الجلسة: {missing}\\nشغّل خلايا التدريب أولًا.\")\n",
    "\n",
    "def eval_rmse_mae(y_true_log, y_pred_log, use_log=True):\n",
    "    if use_log:\n",
    "        y_true = np.expm1(y_true_log); y_pred = np.expm1(y_pred_log)\n",
    "    else:\n",
    "        y_true = y_true_log; y_pred = y_pred_log\n",
    "    rmse = mean_squared_error(y_true, y_pred) ** 0.5\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    return rmse, mae\n",
    "\n",
    "# ---------- (1) تنبؤات validation من النموذجين ----------\n",
    "# LightGBM val\n",
    "lgb_best_iter = int(getattr(lgbm, \"best_iteration_\", 0) or lgbm.get_params().get(\"n_estimators\", 2000))\n",
    "pred_lgb_val  = lgbm.predict(X_val, num_iteration=lgb_best_iter)\n",
    "\n",
    "# XGBoost val (native API): استخرج best_iteration من الخصائص\n",
    "best_iter_attr = (bst.attributes().get(\"best_iteration\") if hasattr(bst, \"attributes\") else None)\n",
    "xgb_best_iter  = int(best_iter_attr) if best_iter_attr is not None else 0\n",
    "dval           = xgb.DMatrix(X_val, label=y_val)\n",
    "pred_xgb_val   = bst.predict(dval, iteration_range=(0, xgb_best_iter)) if xgb_best_iter > 0 else bst.predict(dval)\n",
    "\n",
    "# ---------- (2) البحث عن أفضل وزن على validation ----------\n",
    "weights = np.linspace(0, 1, 21)  # 0.00, 0.05, ..., 1.00\n",
    "best_w = None; best_rmse = 1e18; best_mae = 1e18\n",
    "rows = []\n",
    "for w in weights:\n",
    "    mix = w*pred_xgb_val + (1-w)*pred_lgb_val\n",
    "    rmse, mae = eval_rmse_mae(y_val, mix, use_log=use_log_target)\n",
    "    rows.append((w, rmse, mae))\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse, best_mae, best_w = rmse, mae, w\n",
    "\n",
    "ens_tbl = pd.DataFrame(rows, columns=[\"w_xgb\",\"RMSE\",\"MAE\"]).sort_values(\"RMSE\")\n",
    "print(\"Top ensemble weights on validation:\")\n",
    "print(ens_tbl.head(6).to_string(index=False))\n",
    "print(f\"\\n🏆 Best weight: w_xgb={best_w:.2f} → RMSE={best_rmse:.2f}, MAE={best_mae:.2f}\")\n",
    "\n",
    "# ---------- (3) إعادة التدريب على كامل البيانات ----------\n",
    "print(\"\\nإعادة تدريب LightGBM على كامل البيانات...\")\n",
    "lgb_full = lgb.LGBMRegressor(\n",
    "    n_estimators=lgb_best_iter,\n",
    "    learning_rate=lgbm.get_params().get(\"learning_rate\", 0.05),\n",
    "    num_leaves=lgbm.get_params().get(\"num_leaves\", 64),\n",
    "    min_child_samples=lgbm.get_params().get(\"min_child_samples\", 20),\n",
    "    subsample=lgbm.get_params().get(\"subsample\", 0.8),\n",
    "    colsample_bytree=lgbm.get_params().get(\"colsample_bytree\", 0.8),\n",
    "    random_state=42, n_jobs=-1\n",
    ").fit(X, y)\n",
    "pred_lgb_test = lgb_full.predict(X_test, num_iteration=lgb_best_iter)\n",
    "\n",
    "print(\"إعادة تدريب XGBoost (native) على كامل البيانات...\")\n",
    "dtrain_full = xgb.DMatrix(X, label=y)\n",
    "dtest       = xgb.DMatrix(X_test)\n",
    "# استخدم نفس الإعدادات التي درّبت بها bst من قبل\n",
    "xgb_refit_rounds = xgb_best_iter if xgb_best_iter > 0 else xgb_params.get(\"num_boost_round\", 2000)\n",
    "bst_full = xgb.train(params=xgb_params, dtrain=dtrain_full, num_boost_round=int(xgb_refit_rounds), verbose_eval=False)\n",
    "pred_xgb_test = bst_full.predict(dtest, iteration_range=(0, int(xgb_refit_rounds)))\n",
    "\n",
    "# ---------- (4) المزج بالوزن الأفضل ----------\n",
    "pred_test_ens = best_w*pred_xgb_test + (1-best_w)*pred_lgb_test\n",
    "if use_log_target:\n",
    "    pred_test_ens = np.expm1(pred_test_ens)\n",
    "pred_test_ens = np.clip(pred_test_ens, 1e-6, None)\n",
    "\n",
    "# ---------- (5) بناء ملف التسليم ----------\n",
    "if os.path.exists(sample_path):\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    tgt_cols = [c for c in sample.columns if c.lower() != \"id\"]\n",
    "    sub_tgt  = tgt_cols[0] if tgt_cols else \"trip_duration\"\n",
    "    if id_col and id_col in test.columns and \"id\" in sample.columns:\n",
    "        out = pd.DataFrame({id_col: test[id_col].values, sub_tgt: pred_test_ens})\n",
    "        sub = sample.drop(columns=[sub_tgt], errors=\"ignore\").merge(out, left_on=\"id\", right_on=id_col, how=\"left\")\n",
    "        sub = sub[[\"id\", sub_tgt]]\n",
    "    else:\n",
    "        sub = pd.DataFrame({\"id\": np.arange(len(pred_test_ens)), sub_tgt: pred_test_ens})\n",
    "else:\n",
    "    sub_tgt = \"trip_duration\"\n",
    "    if id_col and id_col in test.columns:\n",
    "        sub = pd.DataFrame({id_col: test[id_col].values, sub_tgt: pred_test_ens}).rename(columns={id_col: \"id\"})\n",
    "    else:\n",
    "        sub = pd.DataFrame({\"id\": np.arange(len(pred_test_ens)), sub_tgt: pred_test_ens})\n",
    "\n",
    "out_path = os.path.join(BASE, \"submission_ensemble_final.csv\")\n",
    "sub.to_csv(out_path, index=False)\n",
    "print(f\"\\n✅ Ensemble submission saved to: {out_path}\")\n",
    "try:\n",
    "    display(sub.head())\n",
    "except:\n",
    "    print(sub.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b3c0cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  trip_duration\n",
      "0  id3004672     774.742326\n",
      "1  id3505355     832.496301\n",
      "2  id1217141     440.746434\n",
      "3  id2150126    1029.464260\n",
      "4  id1598245     337.084151\n",
      "Index(['id', 'trip_duration'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sub = pd.read_csv(r\"C:\\New folder7\\submission_ensemble_final.csv\")\n",
    "print(sub.head())\n",
    "print(sub.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34fbb0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               0\n",
      "trip_duration    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sub.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d2cbde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.220870498656546 4428.192878671506\n"
     ]
    }
   ],
   "source": [
    "print(sub['trip_duration'].min(), sub['trip_duration'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7319212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --quiet!\n"
     ]
    }
   ],
   "source": [
    "!pip install meteostat pytz lightgbm --quiet!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0e528d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather (UTC naive): 2016-01-01 05:00:00 → 2016-07-01 03:00:00\n",
      "Weather rows: 4367\n",
      "\n",
      "✅ Saved: C:\\New folder7\\submission_with_weather.csv\n",
      "          id  trip_duration  temp  prcp  snow   wdir  wspd    pres  rhum  \\\n",
      "0  id3004672     774.742326  24.4   0.0  <NA>  160.0   9.4  1016.0  62.0   \n",
      "1  id3505355     832.496301  24.4   0.0  <NA>  160.0   9.4  1016.0  62.0   \n",
      "2  id1217141     440.746434  24.4   0.0  <NA>  160.0   9.4  1016.0  62.0   \n",
      "3  id2150126    1029.464260  24.4   0.0  <NA>  160.0   9.4  1016.0  62.0   \n",
      "4  id1598245     337.084151  24.4   0.0  <NA>  160.0   9.4  1016.0  62.0   \n",
      "\n",
      "   is_rain  is_snow  \n",
      "0        0        0  \n",
      "1        0        0  \n",
      "2        0        0  \n",
      "3        0        0  \n",
      "4        0        0  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from meteostat import Hourly, Point\n",
    "\n",
    "# -------- إعداد المسارات --------\n",
    "DATA_DIR = r\"C:\\New folder7\"\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"test.csv\")                         # يجب أن يحتوي id, pickup_datetime\n",
    "SUB_PATH  = os.path.join(DATA_DIR, \"submission_ensemble_final.csv\")    # يحتوي id, trip_duration (المتنبأ)\n",
    "OUT_PATH  = os.path.join(DATA_DIR, \"submission_with_weather.csv\")      # الناتج\n",
    "\n",
    "# -------- قراءة الملفات --------\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "sub  = pd.read_csv(SUB_PATH)\n",
    "\n",
    "assert \"id\" in test.columns and \"pickup_datetime\" in test.columns, \"test.csv يجب أن يحوي الأعمدة: id, pickup_datetime\"\n",
    "assert \"id\" in sub.columns, \"ملف التنبؤ يجب أن يحوي العمود id\"\n",
    "\n",
    "# -------- مناطق زمنية --------\n",
    "NY_TZ  = pytz.timezone(\"America/New_York\")\n",
    "UTC_TZ = pytz.UTC\n",
    "\n",
    "# -------- تجهيز وقت الالتقاط في test --------\n",
    "test[\"pickup_datetime\"] = pd.to_datetime(test[\"pickup_datetime\"], errors=\"coerce\")\n",
    "\n",
    "# لو العمود بدون منطقة زمنية ⇒ نلصّق نيويورك، ثم نأخذ رأس الساعة\n",
    "def to_ny_hourly(dt_series):\n",
    "    s = dt_series.copy()\n",
    "    if s.dt.tz is None:\n",
    "        s = s.dt.tz_localize(NY_TZ, nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
    "    else:\n",
    "        s = s.dt.tz_convert(NY_TZ)\n",
    "    return s.dt.floor(\"H\")\n",
    "\n",
    "test[\"pickup_hour_ny\"] = to_ny_hourly(test[\"pickup_datetime\"])\n",
    "\n",
    "# ===== نطاق الطقس الذي سنطلبه من meteostat (UTC) =====\n",
    "# نشتق نفس الأوقات لكن بــ UTC (مع ضمان الوعي بالمنطقة الزمنية)\n",
    "def to_utc_hourly(dt_series):\n",
    "    s = dt_series.copy()\n",
    "    if s.dt.tz is None:\n",
    "        s = s.dt.tz_localize(NY_TZ, nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
    "    return s.dt.tz_convert(UTC_TZ).dt.floor(\"H\")\n",
    "\n",
    "pickup_hour_utc = to_utc_hourly(test[\"pickup_datetime\"])\n",
    "start_utc = pickup_hour_utc.min()\n",
    "end_utc   = pickup_hour_utc.max()\n",
    "\n",
    "# meteostat لا يقبل tz-aware في بعض المقارنات الداخلية ⇒ نحول إلى naive (بدون tzinfo)\n",
    "start_utc_naive = start_utc.tz_localize(None).to_pydatetime()\n",
    "end_utc_naive   = end_utc.tz_localize(None).to_pydatetime()\n",
    "\n",
    "print(\"Fetching weather (UTC naive):\", start_utc_naive, \"→\", end_utc_naive)\n",
    "\n",
    "# ===== جلب الطقس =====\n",
    "# نستخدم محطة Central Park لتمثيل طقس نيويورك\n",
    "station = Point(40.7812, -73.9665)\n",
    "wx = Hourly(station, start_utc_naive, end_utc_naive).fetch().reset_index()  # عمود time قد يكون naive أو aware\n",
    "\n",
    "# --- ضمان أن عمود time «واعٍ» بــ UTC (لو كان naive نجعله UTC) ---\n",
    "if wx[\"time\"].dt.tz is None:\n",
    "    wx[\"time\"] = wx[\"time\"].dt.tz_localize(UTC_TZ)\n",
    "\n",
    "# --- تحويل الوقت إلى نيويورك وأخذ رأس الساعة للدمج ---\n",
    "keep = [\"time\",\"temp\",\"prcp\",\"snow\",\"wdir\",\"wspd\",\"pres\",\"rhum\"]\n",
    "wx = wx[keep].copy()\n",
    "wx[\"pickup_hour_ny\"] = wx[\"time\"].dt.tz_convert(NY_TZ).dt.floor(\"H\")\n",
    "wx.drop(columns=[\"time\"], inplace=True)\n",
    "\n",
    "print(\"Weather rows:\", len(wx))\n",
    "\n",
    "# ===== الدمج على ساعة الالتقاط =====\n",
    "test_w = test.merge(wx, on=\"pickup_hour_ny\", how=\"left\")\n",
    "\n",
    "# تعبئة فراغات بسيطة\n",
    "for c in [\"temp\",\"prcp\",\"snow\",\"wdir\",\"wspd\",\"pres\",\"rhum\"]:\n",
    "    if c in test_w.columns:\n",
    "        test_w[c] = test_w[c].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "# أعلام مبسطة (اختياري)\n",
    "test_w[\"is_rain\"] = (test_w[\"prcp\"].fillna(0) > 0).astype(int)\n",
    "test_w[\"is_snow\"] = (test_w[\"snow\"].fillna(0) > 0).astype(int)\n",
    "\n",
    "# ===== دمج أعمدة الطقس مع ملف التنبؤ حسب id =====\n",
    "sub_w = sub.merge(\n",
    "    test_w[[\"id\",\"temp\",\"prcp\",\"snow\",\"wdir\",\"wspd\",\"pres\",\"rhum\",\"is_rain\",\"is_snow\"]],\n",
    "    on=\"id\", how=\"left\"\n",
    ")\n",
    "\n",
    "# حفظ الناتج\n",
    "sub_w.to_csv(OUT_PATH, index=False)\n",
    "print(f\"\\n✅ Saved: {OUT_PATH}\")\n",
    "print(sub_w.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "863a04f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: 'xgboost--quiet!': Expected end or semicolon (after name and no valid version specifier)\n",
      "    xgboost--quiet!\n",
      "                  ^\n"
     ]
    }
   ],
   "source": [
    "pip install meteostat pytz lightgbm xgboost--quiet!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e238f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install meteostat pytz lightgbm xgboost --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00433153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛅ Fetching weather UTC: 2016-01-01 05:00:00+00:00 → 2016-07-01 03:00:00+00:00\n",
      "Weather shape: (4367, 8)\n",
      "Features used: 15\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.800492 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1611\n",
      "[LightGBM] [Info] Number of data points in the train set: 1166915, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 959.273585\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\tvalid_0's l2: 1.11545e+07\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's l2: 1.04712e+07\n",
      "\n",
      "LGBM  → MAE=536.06 | RMSE=3235.93 | best_iter=12\n",
      "[0]\ttrain-rmse:5562.34838\tvalid-rmse:3252.87813\n",
      "[200]\ttrain-rmse:3021.09939\tvalid-rmse:3525.78242\n",
      "[203]\ttrain-rmse:3018.78873\tvalid-rmse:3525.53109\n",
      "\n",
      "XGB (native) → MAE=442.98 | RMSE=3526.50 | best_iter=4\n",
      "\n",
      "★ Best ensemble weight on val: w_xgb=0.05 → RMSE=3234.84, MAE=527.27\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.201185 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1612\n",
      "[LightGBM] [Info] Number of data points in the train set: 1458644, number of used features: 13\n",
      "[LightGBM] [Info] Start training from score 959.492273\n",
      "\n",
      "✅ Saved ensemble submission: C:\\New folder7\\submission_ensemble_weather.csv\n",
      "          id  trip_duration\n",
      "0  id3004672     876.883841\n",
      "1  id3505355     903.132039\n",
      "2  id1217141     821.796428\n",
      "3  id2150126    1111.034621\n",
      "4  id1598245     838.946931\n"
     ]
    }
   ],
   "source": [
    "# ===================== الإعدادات والحزم =====================\n",
    "import os, numpy as np, pandas as pd, pytz, warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from meteostat import Hourly, Point\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# مسارات الملفات (عدّل المسار عند الحاجة)\n",
    "DATA_DIR   = r\"C:\\New folder7\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_PATH  = os.path.join(DATA_DIR, \"test.csv\")\n",
    "SUB_OUT    = os.path.join(DATA_DIR, \"submission_ensemble_weather.csv\")\n",
    "\n",
    "# المنطقة الزمنية ونقطة الطقس (سنترال بارك، نيويورك)\n",
    "NY_TZ  = pytz.timezone(\"America/New_York\")\n",
    "UTC_TZ = pytz.UTC\n",
    "STATION = Point(40.7812, -73.9665)\n",
    "\n",
    "# ===================== دوال مساعدة =====================\n",
    "def to_ny_hourly(dt_series):\n",
    "    s = pd.to_datetime(dt_series, errors=\"coerce\")\n",
    "    s = s.dt.tz_localize(NY_TZ, nonexistent=\"shift_forward\", ambiguous=\"NaT\") if s.dt.tz is None else s.dt.tz_convert(NY_TZ)\n",
    "    return s.dt.floor(\"H\")\n",
    "\n",
    "def to_utc_hourly(dt_series):\n",
    "    s = pd.to_datetime(dt_series, errors=\"coerce\")\n",
    "    if s.dt.tz is None:\n",
    "        s = s.dt.tz_localize(NY_TZ, nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
    "    return s.dt.tz_convert(UTC_TZ).dt.floor(\"H\")\n",
    "\n",
    "def fetch_weather_for_range(start_utc_hour, end_utc_hour):\n",
    "    # meteostat يفضّل naive داخليًا\n",
    "    start_naive = start_utc_hour.tz_localize(None).to_pydatetime()\n",
    "    end_naive   = end_utc_hour.tz_localize(None).to_pydatetime()\n",
    "    w = Hourly(STATION, start_naive, end_naive).fetch().reset_index()\n",
    "    if w[\"time\"].dt.tz is None:\n",
    "        w[\"time\"] = w[\"time\"].dt.tz_localize(UTC_TZ)\n",
    "    keep = [\"time\",\"temp\",\"prcp\",\"snow\",\"wdir\",\"wspd\",\"pres\",\"rhum\"]\n",
    "    w = w[keep].copy()\n",
    "    w[\"pickup_hour_ny\"] = w[\"time\"].dt.tz_convert(NY_TZ).dt.floor(\"H\")\n",
    "    w.drop(columns=[\"time\"], inplace=True)\n",
    "    return w\n",
    "\n",
    "def attach_weather(df, weather):\n",
    "    out = df.copy()\n",
    "    out[\"pickup_hour_ny\"] = to_ny_hourly(out[\"pickup_datetime\"])\n",
    "    out = out.merge(weather, on=\"pickup_hour_ny\", how=\"left\")\n",
    "    # تعبئة بسيطة\n",
    "    for c in [\"temp\",\"prcp\",\"snow\",\"wdir\",\"wspd\",\"pres\",\"rhum\"]:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    out[\"is_rain\"] = (out[\"prcp\"].fillna(0) > 0).astype(int)\n",
    "    out[\"is_snow\"] = (out[\"snow\"].fillna(0) > 0).astype(int)\n",
    "    return out\n",
    "\n",
    "def eval_scores(y_true, y_pred, is_log=False):\n",
    "    y_t = np.expm1(y_true) if is_log else y_true\n",
    "    y_p = np.expm1(y_pred) if is_log else y_pred\n",
    "    mae  = mean_absolute_error(y_t, y_p)\n",
    "    rmse =mean_squared_error(y_true, y_pred) ** 0.5\n",
    "    return mae, rmse\n",
    "\n",
    "# ===================== 1) قراءة البيانات =====================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "assert \"pickup_datetime\" in train.columns and \"pickup_datetime\" in test.columns, \"يجب وجود pickup_datetime في train/test\"\n",
    "\n",
    "target_col = \"target_log1p\" if \"target_log1p\" in train.columns else \"trip_duration\"\n",
    "id_col = \"id\" if \"id\" in test.columns else None\n",
    "use_log = (target_col == \"target_log1p\")\n",
    "\n",
    "# ===================== 2) جلب الطقس ودمجه =====================\n",
    "train_hour_utc = to_utc_hourly(train[\"pickup_datetime\"])\n",
    "test_hour_utc  = to_utc_hourly(test[\"pickup_datetime\"])\n",
    "start_utc = min(train_hour_utc.min(), test_hour_utc.min())\n",
    "end_utc   = max(train_hour_utc.max(), test_hour_utc.max())\n",
    "\n",
    "print(\"⛅ Fetching weather UTC:\", start_utc, \"→\", end_utc)\n",
    "weather = fetch_weather_for_range(start_utc, end_utc)\n",
    "print(\"Weather shape:\", weather.shape)\n",
    "\n",
    "train_w = attach_weather(train, weather)\n",
    "test_w  = attach_weather(test,  weather)\n",
    "\n",
    "# ===================== 3) تقسيم train إلى train/val =====================\n",
    "trn_df, val_df = train_test_split(train_w, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "drop_train_cols = [c for c in [target_col, id_col, \"pickup_datetime\", \"pickup_hour_ny\"] if c in trn_df.columns]\n",
    "drop_val_cols   = [c for c in [target_col, id_col, \"pickup_datetime\", \"pickup_hour_ny\"] if c in val_df.columns]\n",
    "\n",
    "X_tr = trn_df.drop(columns=drop_train_cols).select_dtypes(include=[np.number])\n",
    "y_tr = trn_df[target_col].values\n",
    "X_val = val_df.drop(columns=drop_val_cols).select_dtypes(include=[np.number])\n",
    "y_val = val_df[target_col].values\n",
    "\n",
    "# توحيد الأعمدة بين train/val\n",
    "common_cols = sorted(set(X_tr.columns) & set(X_val.columns))\n",
    "X_tr, X_val = X_tr[common_cols], X_val[common_cols]\n",
    "print(\"Features used:\", len(common_cols))\n",
    "\n",
    "# ===================== 4) LightGBM مع إيقاف مبكر =====================\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=64,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgbm.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=\"l2\",\n",
    "    callbacks=[lgb.early_stopping(200), lgb.log_evaluation(200)]\n",
    ")\n",
    "lgb_best_iter = int(getattr(lgbm, \"best_iteration_\", 0) or 5000)\n",
    "pred_val_lgb = lgbm.predict(X_val, num_iteration=lgb_best_iter)\n",
    "mae_lgb, rmse_lgb = eval_scores(y_val, pred_val_lgb, use_log)\n",
    "print(f\"\\nLGBM  → MAE={mae_lgb:.2f} | RMSE={rmse_lgb:.2f} | best_iter={lgb_best_iter}\")\n",
    "\n",
    "# ===================== 5) XGBoost (واجهة أصلية xgboost.train) =====================\n",
    "dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "dvalid = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 8,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"seed\": 42\n",
    "}\n",
    "watchlist = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "bst = xgb.train(\n",
    "    params=xgb_params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=6000,\n",
    "    evals=watchlist,\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=200\n",
    ")\n",
    "\n",
    "# التنبؤ على validation\n",
    "best_iter = getattr(bst, \"best_iteration\", None)\n",
    "best_ntree_limit = getattr(bst, \"best_ntree_limit\", None)\n",
    "pred_val_xgb = bst.predict(dvalid, ntree_limit=best_ntree_limit) if best_ntree_limit else bst.predict(dvalid)\n",
    "\n",
    "mae_xgb, rmse_xgb = eval_scores(y_val, pred_val_xgb, use_log)\n",
    "print(f\"\\nXGB (native) → MAE={mae_xgb:.2f} | RMSE={rmse_xgb:.2f} | best_iter={best_iter}\")\n",
    "\n",
    "# ===================== 6) اختيار وزن التجميعة على validation =====================\n",
    "ws = np.arange(0.05, 0.96, 0.05)  # وزن XGB\n",
    "best_w = None\n",
    "for w in ws:\n",
    "    blend = w * pred_val_xgb + (1 - w) * pred_val_lgb\n",
    "    mae_b, rmse_b = eval_scores(y_val, blend, use_log)\n",
    "    if best_w is None or rmse_b < best_w[1]:\n",
    "        best_w = (w, rmse_b, mae_b)\n",
    "print(f\"\\n★ Best ensemble weight on val: w_xgb={best_w[0]:.2f} → RMSE={best_w[1]:.2f}, MAE={best_w[2]:.2f}\")\n",
    "\n",
    "# ===================== 7) تدريب نهائي على كامل train والتنبؤ على test =====================\n",
    "drop_all_cols  = [c for c in [target_col, id_col, \"pickup_datetime\", \"pickup_hour_ny\"] if c in train_w.columns]\n",
    "X_all = train_w.drop(columns=drop_all_cols).select_dtypes(include=[np.number])[common_cols]\n",
    "y_all = train_w[target_col].values\n",
    "\n",
    "# LightGBM كامل\n",
    "lgbm_full = lgb.LGBMRegressor(\n",
    "    n_estimators=lgb_best_iter if lgb_best_iter>0 else 3000,\n",
    "    learning_rate=0.05, num_leaves=64, min_child_samples=20,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1\n",
    ")\n",
    "lgbm_full.fit(X_all, y_all)\n",
    "\n",
    "# XGBoost كامل بنفس أفضل عدد جولات\n",
    "best_rounds = int(best_iter) if isinstance(best_iter, int) and best_iter else 3000\n",
    "dall  = xgb.DMatrix(X_all,  label=y_all)\n",
    "bst_full = xgb.train(\n",
    "    params=xgb_params,\n",
    "    dtrain=dall,\n",
    "    num_boost_round=best_rounds,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# تجهيز test\n",
    "drop_test_cols = [c for c in [id_col, \"pickup_datetime\", \"pickup_hour_ny\"] if c in test_w.columns]\n",
    "X_test = test_w.drop(columns=drop_test_cols).select_dtypes(include=[np.number])\n",
    "X_test = X_test[common_cols]\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "pred_lgb_t = lgbm_full.predict(X_test)\n",
    "pred_xgb_t = bst_full.predict(dtest)\n",
    "\n",
    "w = best_w[0]\n",
    "pred_blend = w * pred_xgb_t + (1 - w) * pred_lgb_t\n",
    "if use_log:\n",
    "    pred_blend = np.expm1(pred_blend)\n",
    "\n",
    "# ===================== 8) حفظ ملف التسليم =====================\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": test[id_col] if id_col else np.arange(len(test)),\n",
    "    \"trip_duration\": pred_blend\n",
    "})\n",
    "sub.to_csv(SUB_OUT, index=False)\n",
    "print(f\"\\n✅ Saved ensemble submission: {SUB_OUT}\")\n",
    "print(sub.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
